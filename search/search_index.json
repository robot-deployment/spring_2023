{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-robot-deployment","title":"Welcome to Robot Deployment","text":"<p>This site was generated 12-1-2023</p>"},{"location":"course-info/links/","title":"Links","text":""},{"location":"course-info/links/#turtlebot-resources","title":"Turtlebot resources","text":"<ul> <li>manual https://turtlebot.github.io/turtlebot4-user-manual/</li> <li>manual repository</li> <li>https://clearpathrobotics.com/turtlebot-4/</li> <li>turtlebot github site https://github.com/turtlebot</li> <li>images: http://download.ros.org/downloads/turtlebot4/</li> <li>unboxing: https://www.youtube.com/watch?v=QN01AXjoLdQ</li> <li>driving: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/driving.html</li> </ul>"},{"location":"course-info/links/#irobot-create3-documentation","title":"IRobot Create3 Documentation","text":"<ul> <li> <p>https://iroboteducation.github.io/create3_docs/</p> </li> <li> <p>https://iroboteducation.github.io/create3_docs/hw/overview/</p> </li> </ul>"},{"location":"course-info/links/#ros2-galactic","title":"Ros2 Galactic","text":"<ul> <li>https://docs.ros.org/en/galactic/index.html</li> <li>https://answers.ros.org/question/292566/what-is-the-difference-between-local_setupbash-and-setupbash/</li> </ul>"},{"location":"course-info/links/#raspberry-pi4","title":"Raspberry Pi4","text":"<ul> <li>https://www.raspberrypi.com/products/raspberry-pi-4-model-b/</li> <li>https://www.raspberrypi.com/documentation/computers/getting-started.html</li> </ul>"},{"location":"course-info/links/#courses","title":"Courses","text":"<p>https://www.udemy.com/course/ros-for-beginners/?referralCode=F41DAF6F3EA18F324DD7</p>"},{"location":"course-info/links/#whole-websites","title":"Whole Websites","text":"<ul> <li>https://roboticsbackend.com</li> <li>https://nu-msr.github.io/me495_site/</li> <li>https://vnav.mit.edu/</li> <li>https://vnav.mit.edu/labs/lab2/</li> <li>https://vnav.mit.edu/labs/lab3/</li> </ul>"},{"location":"course-info/links/#specific-topics","title":"Specific Topics","text":""},{"location":"course-info/links/#organization","title":"Organization","text":"<p>When you define your whole project without external dependencies:</p> <p>https://www.google.com/search?q=managing+ros+installation+multiple+packages+different+repositories&amp;client=firefox-b-1-d&amp;ei=8k9QY6W6H_DDkPIPuKSX6AI&amp;ved=0ahUKEwil-5i_g-36AhXwIUQIHTjSBS0Q4dUDCA4&amp;uact=5&amp;oq=managing+ros+installation+multiple+packages+different+repositories&amp;gs_lcp=Cgdnd3Mtd2l6EAM6CggAEEcQ1gQQsAM6BQghEKABOggIIRAWEB4QHToFCCEQqwI6BQgAEKIEOgcIIRCgARAKSgQITRgBSgQIQRgASgQIRhgAUIYHWLdBYLpCaAZwAXgAgAHOAYgB3C-SAQYwLjQyLjGYAQCgAQHIAQjAAQE&amp;sclient=gws-wiz</p> <p>https://roboticsbackend.com/package-organization-for-a-ros-stack-best-practices/</p> <p>https://nu-msr.github.io/me495_site/custom_ws.html</p> <p>https://answers.ros.org/question/218498/best-practice-one-git-repo-per-package/ https://docs.ros.org/en/foxy/How-To-Guides/Package-maintainer-guide.html https://www.reddit.com/r/ROS/comments/o1gymd/git_strategies_for_including_other_packages_and/</p>"},{"location":"course-info/links/#linux","title":"Linux","text":"<ul> <li>https://explainshell.com/explain?cmd=nmap+-sn</li> </ul>"},{"location":"course-info/links/#textbooks","title":"Textbooks","text":"<p>http://wiki.ros.org/Books</p> <p>https://www.routledge.com/A-Concise-Introduction-to-Robot-Programming-with-ROS2/Rico/p/book/9781032264653</p> <p>Companion github repository</p>"},{"location":"course-info/links/#papers","title":"Papers","text":"<p>https://www.researchgate.net/publication/340059734_A_Self-Driving_Car_Architecture_in_ROS2/link/5e80bf2b458515efa0b874d8/download</p>"},{"location":"course-info/links/#tutorials","title":"Tutorials","text":"<p>https://maker.pro/ros/tutorial/robot-operating-system-2-ros-2-introduction-and-getting-started</p>"},{"location":"course-info/links/#ros-services","title":"Ros services","text":"<p>https://www.theconstructsim.com/for-campus/</p>"},{"location":"course-info/links/#microros-and-ros-on-esp32","title":"MicroROS and Ros on ESP32","text":"<ul> <li>https://ubuntu.com/blog/getting-started-with-micro-ros-on-raspberry-pi-pico</li> <li>https://blog.hadabot.com/set-up-esp32-microcontroller-for-ros2-robotics.html</li> <li>https://github.com/RobotWebTools/rosbridge_suite</li> <li>Hadabot</li> <li>https://www.hadabot.com/new-user-software-stack-setup.html</li> <li>https://blog.hadabot.com/set-up-esp32-microcontroller-for-ros2-robotics.html</li> <li>https://github.com/hadabot/hadabot_main/blob/master/firmware/uhadabot/uroslibpy/ros.py</li> </ul> <p>## To ROS or not to ROS</p> <ul> <li>https://pulurobotics.fi/blog/pulurobotics-blog-1/post/why-don-t-we-use-ros-7</li> <li>https://www.theconstructsim.com/not-using-ros-robotics-product/</li> </ul>"},{"location":"course-info/syllabus/","title":"Syllabus","text":""},{"location":"course-info/syllabus/#overview","title":"Overview","text":"<p>Experimentation and Deployment of Robotic Systems is a course organized around the principles of robotic data collection, aggregation, interaction, and decision-making so that students can more effectively conduct experimental validation of robotic systems, deploy mobile and autonomous robotic systems, and more closely interact with robots across various types of real and synthetic data streams.  It walks students through the principles of interacting with sensors and hardware, signal conditioning, and model-driven data, building data pipelines for the purposes of data collection, visualization, and control.  This class studies resulting systems, with a focus on communication, data, and decision-making, with application-specific experimental/mobile projects.  </p>"},{"location":"course-info/syllabus/#class-basics","title":"Class Basics","text":""},{"location":"course-info/syllabus/#section-info-and-meeting-times","title":"Section Info and Meeting Times","text":"<p>Class: EGR 598-- Experimentation and Deployment of Robotic Systems Class Schedule:  Monday / Wednesday 1:30pm-2:45pm Meeting Location: Polytechnic Campus, Peralta 103 Course Number:  36207  </p>"},{"location":"course-info/syllabus/#instructor-contact-info","title":"Instructor Contact Info","text":"<p>Instructor: Daniel M. Aukes E-mail: danaukes@asu.edu Instructor Office: Santa Catalina 331C, Polytechnic Campus  </p>"},{"location":"course-info/syllabus/#office-hours","title":"Office Hours","text":"<p>Office Hours will held weekly starting week 2 or may be made by appointment.  I will run a survey to identify the best time to hold office hours within my own constraints.  This document will be updated to reflect the official office hours that start week 2.  This document will be updated to reflect up-to-date office hours as needed.</p>"},{"location":"course-info/syllabus/#prerequisites","title":"Prerequisites","text":"<p>There are no formal prerequisites, but students taking EGR598 should be familiar with:</p> <ul> <li>Programming fundamentals, ideally in a scripted language like Python or Matlab.</li> <li>Linear algebra, differential equations, calculus, trigonometry, vectors, etc.</li> <li>The fundamentals of embedded systems, mechatronics</li> <li>The basics of serial communication, such as TCP/IP and other common ethernet protocols</li> <li>Willing to try Ubuntu</li> </ul>"},{"location":"course-info/syllabus/#course-objectives","title":"Course Objectives","text":"<p>At the end of this course, students will demonstrate proficiency in synthesizing concepts from across a number of engineering domains including robotics, modeling and analysis, optimization, data collection and experimental validation, etc.  This includes:</p>"},{"location":"course-info/syllabus/#expected-learning-outcomes","title":"Expected Learning Outcomes","text":""},{"location":"course-info/syllabus/#python","title":"Python","text":"<ul> <li>You will be able to install various packages in Python</li> <li>You will be able to identify and utilize the key differences between data types in writing Python code.</li> <li>You will be able to create and use functions for the purposes of modularizing Python code</li> <li>You will be able to plot multidimensional tabular data as figures using the matplotlib library.</li> <li>You will be able to demonstrate how to perform array and matrix-based operations using the Numpy and Scipy packages</li> <li>You will be able to use Python to work with graphical user interfaces</li> <li>You will be able to integrate python plotting and data visualization with graphical user interfaces</li> <li>You will be able to add user control to windows and GUIs</li> </ul>"},{"location":"course-info/syllabus/#repositories-and-version-control","title":"Repositories and Version Control","text":"<ul> <li>You will be able to list the basic functions of version control tools like git</li> <li>You will be able to employ the basic functions of version control tools like git</li> <li>You will be able to create a new project in version control tools like git</li> <li>You will be able to structure a new project in version control tools like git using submodules</li> <li>You can demonstrate documenting your project in version control tools like git</li> </ul>"},{"location":"course-info/syllabus/#robotics","title":"Robotics","text":"<ul> <li>You will be able to read from or write to existing hardware data streams in ROS</li> <li>You will be able to develop new data streams for existing off-the-shelf hardware in ROS</li> <li>You will be able to develop data streams for novel hardware in ROS</li> <li>You will be able to develop basic controllers</li> <li>You will be able to develop and deploy higher-level controllers and algorithms for sensing and control.</li> </ul>"},{"location":"course-info/syllabus/#experimentation","title":"Experimentation","text":"<ul> <li>You will understand best practices for developing an experiment</li> <li>Understand ways to reduce noise and variation</li> <li>Develop a small experiment and collect data</li> <li>Interpret sources of error and corrective actions</li> </ul>"},{"location":"course-info/syllabus/#robot-hardware-and-communiation","title":"Robot Hardware and Communiation","text":"<ul> <li>you will be able to install tools like the Robot Operating System (ROS)</li> <li>You will be able to publish and subscribe to topics through a communication layer such as ROS</li> <li>You will be able to create and deploy nodes and services coded in languages like Python and C++</li> </ul>"},{"location":"course-info/syllabus/#team-based-project-management-communication-etc","title":"Team-based project management, communication, etc","text":"<ul> <li>Develop a project website for communicating progress in various forms</li> <li>Present work orally to the class</li> <li>Present work in written form.</li> <li>Present data usefully, in graphical forms.</li> <li>Present work through edited videos.</li> </ul>"},{"location":"course-info/syllabus/#textbook-materials-equipment-and-personal-laptops","title":"Textbook, Materials, Equipment, and Personal Laptops","text":""},{"location":"course-info/syllabus/#textbook","title":"Textbook","text":"<p>A Concise Introduction to Robot Programming with ROS2(recommended) Francisco Mart\u00edn Rico 1st Edition eBook ISBN 9781003289623  </p>"},{"location":"course-info/syllabus/#software","title":"Software","text":"<p>You will be expected to install and use the following software (not a complete list)</p> <ul> <li>Virtualbox</li> <li>In a Virtual Machine:<ul> <li>Ubuntu 20.04</li> <li>ROS2 (Galactic)</li> <li>various other programs</li> </ul> </li> </ul> <p>This class is friendly to all operating systems.  Students have used Windows, Ubuntu or OS/X on their own in the past with no problems.</p>"},{"location":"course-info/syllabus/#computers","title":"Computers","text":"<p>It is expected that you can bring a laptop to class to complete in-class programming tasks.  </p>"},{"location":"course-info/syllabus/#hardware-specs","title":"Hardware Specs","text":"<ul> <li>8Gb RAM</li> <li>minimum 50Gb free space</li> </ul>"},{"location":"course-info/syllabus/#peripheral-requirements","title":"Peripheral Requirements","text":"<ul> <li>Admin access to home router with both 2.4GHz and 5GHz available</li> <li>Computer monitor</li> <li>4-port USB hub</li> <li>wired ethernet port or USB dongle (optional)</li> <li>various USB cables (micro/mini/USB-C)</li> </ul>"},{"location":"course-info/syllabus/#other-equipment","title":"Other Equipment","text":"<p>Special tools and equipment required for completeing this class will be made available for use on the Poly Campus.  </p>"},{"location":"course-info/syllabus/#checkout","title":"Checkout","text":"<p>Checkout of equipment or reusable parts may be possible through Dr. Aukes, the Innovation Hub, or Peralta Labs.  Any checked-out tools or parts must be returned in order to receive a grade in the class.</p>"},{"location":"course-info/syllabus/#other-recommended-resources","title":"Other recommended resources","text":"<ul> <li>Adobe Creative Cloud, available to all ASU students for free: https://uto.asu.edu/adobe-creative-cloud.</li> <li>Microsoft Office (Microsoft 365 is free for all currently-enrolled ASU students)</li> <li>Solidwoks, available via https://myapps.asu.edu</li> </ul>"},{"location":"course-info/syllabus/#project","title":"Project","text":"<p>The final project will involve developing and deploying a robotic system using the methods introduced in this class.   The project will span the last few weeks of the semester.</p>"},{"location":"course-info/syllabus/#class-schedule","title":"Class Schedule","text":"<p>The class schedule can be found on Canvas.  It is subject to change, and will be updated regularly. It is your responsibility to keep track of all due dates and times, which will all be found on canvas.</p>"},{"location":"course-info/syllabus/#tentative-schedule1","title":"Tentative Schedule\\(^1\\)","text":"Lecture Topic 1 Welcome 2 Linux I 3 Linux II 4 Linux III 5 Git &amp; Github 6 Python I 7 Python II 8 Python III 9 Python IV 10 Python V 11 ROS I: Introduction 12 ROS II: Tutorial 1 13 ROS III: Topics 14 ROS IV: Nodes and Services 15 ROS V: GUIs and Bag Files 16 Ethernet and Networking 17 Hardware I: Cameras 18 Hardware II: Serial Data 19 Hardware III: Robots 20 Hardware IV: TurtleBot 21 Filtering Data 22 Control 23 Project Time 24 Computer Vision 25 Project Time 26 Machine Learning / AI 27 Project Time 28 Project Time 29 Project Time 30 Final Presentations <p>\\(^1\\) The Schedule is subject to change with appropriate warning.</p>"},{"location":"course-info/syllabus/#assignments","title":"Assignments","text":""},{"location":"course-info/syllabus/#tentative-assignment-schedule-3","title":"Tentative Assignment Schedule \\(^3\\)","text":"<p>Expect roughly one assignment per week, consisting of tasks to complete at home related to the week's lecture topics.</p> <p>\\(^3\\) Assignments, totals, relative weighting, and due dates are subject to change with appropriate warning.  Please see Canvas for the current schedule.</p>"},{"location":"course-info/syllabus/#how-to-succeed-in-this-course","title":"How to Succeed in this Course","text":"<ul> <li>Attend all class sessions</li> <li>Complete all pre-class preparation assignments and reading</li> <li>Complete all post-class follow up assignments and reading</li> <li>Participate in office hours</li> <li>Check your ASU email regularly</li> <li>Log in to the Canvas at least once each week</li> <li>Communicate proactively with your instructor</li> <li>Create a study schedule so that you don\u2019t fall behind on assignments</li> </ul>"},{"location":"course-info/syllabus/#grading-policies","title":"Grading Policies","text":"<p>The goal of assignments is to develop a fundamental understanding of the topics required to deploy robotic systems in experiment designs and in the field.</p>"},{"location":"course-info/syllabus/#assignments-are-on-canvas","title":"Assignments are on Canvas","text":"<p>Assignments will be posted to Canvas throughout the semester.  It is the student's responsibility to check canvas periodically for announcements and posted material.  Assignments will cover many of the topics presented in class.  </p>"},{"location":"course-info/syllabus/#type-of-assignments","title":"Type of Assignments","text":"<p>Assigned work may be in the form of a longer-form, weekly assignment intended to teach a new fundamental skill, or it may be a short, small-point-value assignment consisting of tasks that must be completed in order for you to complete other milestones.  Some surveys also have a small number of points assigned to them, to ensure student participation.  In-class work generally serves as a starting point for assigned homework and is typically ungraded, though it may be graded occasionally.  </p> <p>Please see the \"Rubric\" section of each assignment for assignment-specific expectations.</p>"},{"location":"course-info/syllabus/#project-vs-individual-assignments","title":"Project vs. Individual Assignments","text":"<p>Assigned work may be individual in nature or team-based project assignments.  Individual assignments will be graded on an individual basis, and are intended to reflect your own work.  Please use the discussion board feature on Canvas when you have a question. Copying others' work is not permitted. </p> <p>The grade for team-based assignments will be shared by all participating members.  </p>"},{"location":"course-info/syllabus/#grading-scale","title":"Grading Scale","text":"<p>Final points will receive a letter grade according to the following table:</p> Grade Range A+ 97-100.0 A 93-96.9 A- 90-92.9 B+ 87-89.9 B 83-86.9 B- 80-82.9 C+ 77-79.9 C 70-76.9 D 60-69.9 E 0-59.9"},{"location":"course-info/syllabus/#grading-rubric","title":"Grading Rubric","text":"<p>Some assignments will be graded according to rubric with number values corresponding to a sliding qualitative scale .  The following is a general description of what each percentage means in this course:</p> Description % Exceeds Expectations. Shows superior effort, quality, mastering of the concepts.  Innovation in the execution of submitted work.  Documentation is publication-ready. 100 Above expectations.  Demonstrates full understanding of the problem, and solution is well executed, documented, and presented. 85 Meets expectations.  Minor mistakes are present, but student demonstrates a general understanding of the concepts.  Documentation present but perhaps not comprehensive. 70 Below expectations. Some effort shown, though there may be serios flaws in analysis or execution.  Documentation lacking in certain areas. 55 Fails to meet minimum expections.  Minimal effort shown.  Does not show understanding and may not have thought through their methods.  Documentation is lacking substance, clarity, completeness, evidence of effort. 40 Not submitted, illegible, not readable, not properly linked 0"},{"location":"course-info/syllabus/#late-penalities","title":"Late Penalities","text":"<p>Due to the nature of this class, failing to turn in an assignment on time affects you and your classmates, as each concept builds on the last.  It is your responsibility to get in touch with the instructor regarding any questions before assignments are due.  Late submissions will lose one letter grade(10%) for every day they are late\\(^4\\).  Any sumbission more than four days late will receive a zero.  Additionally, due to the nature of the submission process, late CATME assignments will not be accepted.</p> <p>All assignments must be submitted to Canvas by the date and time noted in Canvas.</p> <p>\\(^4\\) meaning 10% of the total possible number of points</p>"},{"location":"course-info/syllabus/#submitting-and-presenting-work","title":"Submitting and Presenting work","text":"<p>Assignment submissions must follow the \"Submission Best Practices\" document shared on Canvas.  It outlines the expectations for well written assignments, reports, and presentations.</p> <p>Assigned homework will be submitted for grading several different ways.  This is always indicated in the \"Submission\" section of each assignment.</p> <ul> <li>It may be submitted for grading via Canvas.  </li> <li>Other work involving external tools (Google Surveys, CATME, etc) will be graded based on submitting to that external tool.  </li> <li>Some work will be presented in front of the class, and the grade derived from the presentation.  </li> <li>Other work will be compiled into the design notebook (in the form of a website or report) and graded priodically.</li> </ul> <p>It is the student's responsibility to pay close attention to each assignment's submission instructions, as each assignment indicates the method by which the work must be submitted for grading.  Failure to submit work in the manner asked for in each assignment will result in a zero.</p>"},{"location":"course-info/syllabus/#course-policies","title":"Course Policies","text":""},{"location":"course-info/syllabus/#attendance-participation","title":"Attendance &amp; Participation","text":""},{"location":"course-info/syllabus/#summary","title":"Summary","text":"<ul> <li>Attendance is required.  More than two absences result in -2% grade reduction per missed class.</li> <li>Absences and Tardies are treated the same</li> <li>Coordinate with fellow students to take notes if you are gone.</li> <li>Email me at once of technical difficulties over Zoom, but try to reconnect ASAP using other technology (Hotspot, cell-phone, call-in, etc.)</li> <li>If you are registered, you must attend the first class or I will ask that you drop the course.  This is due to the high demand for a small number of open slots.</li> </ul>"},{"location":"course-info/syllabus/#details","title":"Details","text":"<p>This class is structured so that it can only be successful with your attendance.  Classes will be interactive, and will require you to come with questions, answers, and ideas to discuss.  Students should notify me if they will miss class, although this does not excuse them from learning the concepts or turning in their assignments on time.  </p> <p>Missing more than two classes will result in noticeable penalties to students' grade, in the form of -2% off the student's final grade per missed class over two.\\(^5\\)</p> <p>Please coordinate with your fellow students to make sure someone takes notes during class if you will be unavoidably gone.</p> <p>\\(^5\\) counted as 2% of total points.</p>"},{"location":"course-info/syllabus/#accommodations","title":"Accommodations","text":"<p>Attendance and participation in class activities is an essential part of the learning process, and students are expected to attend class regularly. Some absences are, however, unavoidable. Excused absences for classes will be given without penalty to the grade in the case of (1) a university-sanctioned event ACD 304-02; (2) religious holidays ACD 304-04; a list can be found here; (3) work performed in the line-of-duty according to SSM 201-18; and (4) illness, quarantine or self-isolation related to illness as documented by a health professional.</p> <p>Anticipated absences for university-sanctioned events, religious holidays, or line-of-duty activity should be communicated to the instructor by email at danaukes@asu.edu, at least 2 days before the expected absence.</p> <p>Absences for illness, quarantine or self-isolation related to illness should be documented by a health professional and communicated to the instructor as soon as possible by email at danaukes@asu.edu.</p> <p>Excused absences do not relieve students from responsibility for any part of the course work required during the period of absence. Faculty will provide accommodations that may include participation in classes remotely, access to recordings of class activities, and make-up work.</p> <p>If there is a disagreement as to whether an absence should be accommodated, the instructor and student should contact the academic unit chair immediately for resolution.</p>"},{"location":"course-info/syllabus/#classroom-behavior","title":"Classroom Behavior","text":""},{"location":"course-info/syllabus/#summary_1","title":"Summary","text":"<ul> <li>Keep all communication professional</li> <li>Turn off all cell phones, pagers, and other personal devices when participating in class</li> <li>Use your laptops for classroom activities, not email, chats, web browsing, or other non-class related activities.</li> </ul>"},{"location":"course-info/syllabus/#details_1","title":"Details","text":""},{"location":"course-info/syllabus/#professional-communication","title":"Professional Communication","text":"<p>Professional Communication in all forms is required.  This includes proper dress when attending class remotely and in-person.  Please refrain from using any background images in your zoom video feed, though you should consider blacking out your background for privacy and professionalism.</p>"},{"location":"course-info/syllabus/#cell-phones-pagers-and-other-personal-devices","title":"Cell phones, pagers, and other personal devices","text":"<p>Cell phones, pagers, and other personal devices must be turned off during class to avoid causing distractions.  The use of recording devices is not permitted during class.  Any violent or threatening conduct by an ASU student in this class will be reported to the ASU Police Department and the Office of the Dean of Students.  </p>"},{"location":"course-info/syllabus/#use-of-laptops-in-class","title":"Use of laptops in class","text":"<p>Laptops are strongly suggested for this course.  You may use your laptop to take notes, during tutorial sessions, or when giving presentations.  Please do not use class time for emails, chats, web browsing, or other non-class related activities.</p>"},{"location":"course-info/syllabus/#reorganizing-a-team","title":"Reorganizing a Team","text":""},{"location":"course-info/syllabus/#summary_2","title":"Summary","text":"<ul> <li>Please try to work out any team-based issues.</li> <li>Please see me if the team is not working.  I may choose to split the team</li> </ul>"},{"location":"course-info/syllabus/#details_2","title":"Details","text":"<p>Reorganizing teams is not a desired outcome of a group project but is sometimes necessary if dysfunction rises to a level that it cannot complete the project.  One or more teammates or the instructor may initiate the process to split or reorganize a team.  Splitting teams does not necessarily work in any members' best interests, as team-based Team Assignments, which each team member must contribute to, are afterwards spread across fewer people.</p> <p>However, if the need arises, members must work with the professor to outline the issues which are creating the need to reorganize and the measures which remaining teammates may take to rectify the situation.  This can take the form of changes made to communication, workload reallocation, new meeting times, etc.  The professor will have the final say in establishing a set of expectations for the team, which must be met within a week.  If members fail to live up to these expectations, the team may be split and reorganized, as deemed necessary by the instructor.</p> <p>When reorganiztion occurs, each new team will set up their own folders starting with the former team's work, but  new material will be created by the new team, and old material adapted based on the new direction of each new team.  Any changes to the project definition due to the split (such as project scope, performance specifications, timeline, etc) will need to be coordinated with the instructor for all future submissions or presentations.</p> <p>The instructor has the final say in the establishment and reoganization of teams.</p>"},{"location":"course-info/syllabus/#academic-integrity","title":"Academic Integrity","text":"<p>This class is meant to teach you how to create and use your own design tools for creating folding robots using a variety of published resources, online resources, and classroom content.  I encourage you to plumb the depths of what's available; through this synthesis you might be able to create something unique.  However, I expect to be able to tell what is your work and what is someone else's.  For this reason, specific rules for this class are:</p>"},{"location":"course-info/syllabus/#specific-rules","title":"Specific Rules","text":"<ul> <li>Do your own work for individual assignments and tests.</li> <li>Include the your sources of inspiration within assignments and projects.  This will help grow the list of cool references, but more importantly, help distinguish inspiration from wholesale plagarism.</li> <li>Keep code/text/information you use from outside sources separate from your own original content (through the use of separate folders, for example).  Make it explicit what is yours and what is not.</li> <li>Include all the licenses or copyright statements as required by the things you reuse.  This will make your own code more reuseable for yourself and potentially others in the future.</li> <li>See https://provost.asu.edu/academic-integrity/policy for more info.</li> </ul> <p>Students in this class must adhere to ASU\u2019s academic integrity policy, which can be found at https://provost.asu.edu/academic-integrity/policy. Students are responsible for reviewing this policy and understanding each of the areas in which academic dishonesty can occur. In addition, all engineering students are expected to adhere to both the ASU Academic Integrity Honor Code and the Fulton Schools of Engineering Honor Code. All academic integrity violations will be reported to the Fulton Schools of Engineering Academic Integrity Office (AIO).  The AIO maintains record of all violations and has access to academic integrity violations committed in all other ASU college/schools.</p>"},{"location":"course-info/syllabus/#recordings","title":"Recordings","text":"<p>Note that class sessions may be recorded, and recordings provided to enrolled students, instructors or instructional support personnel as deemed necessary by the course instructor. If you have concerns about being recorded, please contact the course instructor.</p>"},{"location":"course-info/syllabus/#copyright","title":"Copyright","text":"<p>All course content and materials, including lectures (Zoom recorded lectures included), are copyrighted materials and students may not share outside the class, upload to online websites not approved by the instructor, sell, or distribute course content or notes taken during the conduct of the course (see ACD 304\u201306, \"Commercial Note Taking Services\" and ABOR Policy 5-308 F.14 for more information).</p> <p>You must refrain from uploading to any course shell, discussion board, or website used by the course instructor or other course forum, material that is not the student's original work, unless the students first comply with all applicable copyright laws; faculty members reserve the right to delete materials on the grounds of suspected copyright infringement.</p>"},{"location":"course-info/syllabus/#policy-against-threatening-behavior-per-the-student-services-manual-ssm-10402","title":"Policy against threatening behavior, per the Student Services Manual, SSM 104\u201302","text":"<p>Students, faculty, staff, and other individuals do not have an unqualified right of access to university grounds, property, or services. Interfering with the peaceful conduct of university-related business or activities or remaining on campus grounds after a request to leave may be considered a crime. All incidents and allegations of violent or threatening conduct by an ASU student (whether on- or off-campus) must be reported to the ASU Police Department (ASU PD) and the Office of the Dean of Students.</p>"},{"location":"course-info/syllabus/#disability-accommodations","title":"Disability Accommodations","text":"<p>Suitable accommodations will be made for students having disabilities. Students needing accommodations must register with the ASU Disabilities Resource Center and provide documentation of that registration to the instructor. Students should communicate the need for an accommodation in sufficient time for it to be properly arranged. See ACD 304-08 Classroom and Testing Accommodations for Students with Disabilities.  </p> <p>The Americans with Disabilities Act (ADA) is a federal antidiscrimination statute that provides comprehensive civil rights protection for persons with disabilities. One element of this legislation requires that all qualified students with documented disabilities be guaranteed a learning environment that provides for reasonable accommodation of their disabilities. If you believe you have a disability requiring an accommodation please contact the Disability Resource Center at ASU Polytechnic located in Student Affairs Quad # 4 or call 480-727-1039 / TTY: 480-727-1009.  Eligibility and documentation policies are online at:  http://www.asu.edu/studentaffairs/ed/drc/</p>"},{"location":"course-info/syllabus/#harassment-and-sexual-discrimination","title":"Harassment and Sexual Discrimination","text":"<p>Arizona State University is committed to providing an environment free of discrimination, harassment, or retaliation for the entire university community, including all students, faculty members, staff employees, and guests. ASU expressly prohibits discrimination, harassment, and retaliation by employees, students, contractors, or agents of the university based on any protected status: race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, gender identity, and genetic information.</p> <p>Title IX is a federal law that provides that no person be excluded on the basis of sex from participation in, be denied benefits of, or be subjected to discrimination under any education program or activity.  Both Title IX and university policy make clear that sexual violence and harassment based on sex is prohibited.  An individual who believes they have been subjected to sexual violence or harassed on the basis of sex can seek support, including counseling and academic support, from the university.  If you or someone you know has been harassed on the basis of sex or sexually assaulted, you can find information and resources at https://sexualviolenceprevention.asu.edu/faqs.</p> <p>As a mandated reporter, I am obligated to report any information I become aware of regarding alleged acts of sexual discrimination, including sexual violence and dating violence.  ASU Counseling Services, https://eoss.asu.edu/counseling is available if you wish to discuss any concerns confidentially and privately. ASU online students may access 360 Life Services, https://goto.asuonline.asu.edu/success/online-resources.html.</p>"},{"location":"course-info/syllabus/#student-support-services","title":"Student Support Services","text":"<ul> <li>ASU Libraries - offers 24/7 access to librarians through \"Ask a Librarian\" online chat and help by librarians in person at the Reference Desk during most hours the libraries are open. http://www.asu.edu/lib/</li> <li>Counseling and Consultation \u2013 provides confidential mental health and career counseling services for all ASU students. http://www.asu.edu/studentaffairs/counseling/</li> <li>Learning Resource Center \u2013 provides students with academic support services such as tutoring, peer advising, computer assisted instruction, and supplemental instruction.  Offers both free and fee-based services.  http://www.asu.edu/vpsa/lrc/</li> <li>Writing Center \u2013 provides on-site tutors to help students increase their confidence as writers and improve writing skills free of charge.  http://www.asu.edu/duas/wcenter/</li> <li>Career Services \u2013 offers assistance to students in choosing a major, setting career goals, interviewing and job hunting strategies. http://career.asu.edu/</li> <li>Student Financial Aid Office \u2013 offers information and applications for student funding such as grants, loans, scholarships and student employment. http://www.asu.edu/fa/</li> <li>Student Health and Wellness Center \u2013 provides non-emergency medical health care to all ASU students regardless of insurance status. Most visits with a physician or nurse practitioner are free of charge, but fees will be incurred for x-rays, lab results, etc., http://www.asu.edu/health/</li> <li>Student Recreational Center \u2013 offers individual and group fitness opportunities, as well as information on nutrition and wellness, and massages. Use of the general facilities (weights, circuit training and cardio machines) are free, other services (yoga classes, massages) are fee-based.  http://www.asu.edu/src/</li> <li>Student Legal Assistance \u2013 provides legal advice and counsel free of charge to all ASU students in areas such as landlord-tenant law, credit reports and collection issues, taxability of scholarships and grants, etc. Notary service is also available at no charge. http://www.asu.edu/mu/legal/</li> <li>Help Wiki \u2013 provides a frequently asked questions resource for technology users at ASU. http://wiki.asu.edu/help/</li> <li>EMPACT Crisis Hotline \u2013 offers free 24-hour support for mental health crises. Call (480) 784-1500 in the Phoenix area, (866) 205-5229 for the toll-free number outside of Phoenix, and (480) 736-4949 for the sexual assault hotline. All services are free and confidential. http://www.empact-spc.com/</li> </ul>"},{"location":"course-info/syllabus/#notice","title":"Notice","text":"<p>Any information in this syllabus (other than grading and absence policies) may be subject to change with reasonable advance notice.</p>"},{"location":"lecture-notes/02/","title":"Lecture 02 Notes","text":""},{"location":"lecture-notes/02/#virtualbox-links","title":"Virtualbox Links","text":"<ul> <li>tutorial</li> <li>https://danaukes.com/notebook/virtualbox/</li> </ul>"},{"location":"lecture-notes/02/#small-form-factor-linux-pcs","title":"Small Form Factor Linux PCs","text":"<ul> <li>https://www.omgubuntu.co.uk/2012/01/the-ben-nanonote-the-worlds-smallest-linux-laptop</li> <li>https://mashable.com/deals/aug-24-vocore2-mini-linux-computer-bundle</li> <li>https://itsfoss.com/linux-based-mini-pc/</li> </ul>"},{"location":"lecture-notes/02/#terminal-links","title":"Terminal links","text":"<ul> <li>https://superuser.com/questions/880344/what-is-the-difference-between-terminal-and-bash</li> <li>https://www.networxsecurity.org/members-area/glossary/c/computer-terminal.html</li> <li>https://unix.stackexchange.com/questions/180943/terminal-vs-bash</li> </ul>"},{"location":"lecture-notes/02/#terminal-commands","title":"Terminal Commands","text":"<ul> <li>linux basics tutorial</li> <li>My personal notes</li> <li>https://cheatography.com/davechild/cheat-sheets/linux-command-line/</li> <li>https://explainshell.com/</li> </ul>"},{"location":"lecture-notes/03/","title":"Lecture 03 Notes","text":""},{"location":"lecture-notes/03/#general-references","title":"General References","text":"<ul> <li>https://www.linode.com/docs/guides/introduction-to-linux-concepts/</li> <li>https://www.quora.com/What-is-the-main-difference-between-echo-and-cat-in-Linux-shell?share=1</li> <li>processes<ul> <li>https://howtoforge.com/4-cli-based-linux-performance-monitoring-tools-top-htop-ps-and-vmstat/</li> <li>https://www.baeldung.com/linux/htop-ps-more-processes</li> </ul> </li> <li>awk<ul> <li>https://www.tutorialspoint.com/awk/awk_basic_examples.htm</li> </ul> </li> </ul>"},{"location":"lecture-notes/03/#bash-special-characters","title":"Bash special characters","text":"<ul> <li>https://www.howtogeek.com/439199/15-special-characters-you-need-to-know-for-bash/</li> <li>https://stackoverflow.com/questions/12765340/difference-between-parentheses-and-brackets-in-bash-conditionals</li> </ul>"},{"location":"lecture-notes/03/#pipes","title":"Pipes (<code>|</code>)","text":"<ul> <li>https://opensource.com/article/18/10/linux-data-streams</li> </ul>"},{"location":"lecture-notes/03/#variables-and-environment-variables","title":"Variables and Environment Variables","text":"<ul> <li>https://omairmajid.com/posts/2020-06-25-variables-and-environment-variables/</li> </ul>"},{"location":"lecture-notes/03/#misc","title":"Misc","text":"<ul> <li>(echo vs print)</li> </ul>"},{"location":"lecture-notes/03/#file-editing-from-the-command-line","title":"File editing from the command line","text":"<ul> <li>https://www.tecmint.com/linux-command-line-editors/</li> <li>https://vim.rtorr.com/</li> </ul>"},{"location":"lecture-notes/03/#installing-packages","title":"Installing packages","text":"<ul> <li>https://www.howtogeek.com/791055/apt-vs.-apt-get-whats-the-difference-on-linux/</li> <li>https://linuxize.com/post/how-to-add-apt-repository-in-ubuntu/</li> <li>https://help.ubuntu.com/community/Repositories/Ubuntu</li> </ul>"},{"location":"lecture-notes/03/#imu-comparison","title":"IMU Comparison","text":"<ul> <li>MPU6050</li> <li>BNO055</li> <li>BMX160</li> </ul>"},{"location":"lecture-notes/04/","title":"Lecture 04 Notes","text":""},{"location":"lecture-notes/04/#networking-basics","title":"Networking Basics","text":"<ul> <li>https://help.ui.com/hc/en-us/articles/115005968648-Intro-to-Networking-IPv4-Addressing-Subnets</li> <li>https://www.digitalocean.com/community/tutorials/understanding-ip-addresses-subnets-and-cidr-notation-for-networking</li> <li>https://www.freecodecamp.org/news/tcp-vs-udp/</li> <li>TCP VS UDP: https://microchipdeveloper.com/tcpip:tcp-vs-udp</li> <li>https://en.wikipedia.org/wiki/Reserved_IP_addresses</li> </ul>"},{"location":"lecture-notes/04/#default-permissions","title":"Default Permissions","text":"<ul> <li>https://www.tecmint.com/set-ssh-directory-permissions-in-linux/</li> <li>https://superuser.com/questions/215504/permissions-on-private-key-in-ssh-folder</li> </ul>"},{"location":"lecture-notes/04/#create-a-new-ssh-key","title":"Create a new SSH key","text":"<ol> <li> <p>Use a key like \"ed25519\"</p> <p><code>bash ssh-keygen -t ed25519</code></p> </li> <li> <p>Select a passphrase</p> </li> <li> <p>print the public key:</p> <p><code>bash cat ~/.ssh/id_ed25519.pub</code></p> </li> </ol>"},{"location":"lecture-notes/04/#add-your-keys-to-authorized_keys","title":"Add your key(s) to authorized_keys","text":"<p>edit:</p> <pre><code>sudo nano ~/.ssh/authorized_keys\n</code></pre> <p>or copy directly:</p> <pre><code>cat ~/.ssh/id_ed25519.pub &gt;&gt; ~/.ssh/authorized_keys\n</code></pre>"},{"location":"lecture-notes/04/#default-permissions_1","title":"Default Permissions","text":"<pre><code>chmod 700 ~/.ssh # the .ssh directory itself\nchmod 600 ~/.ssh/* # by default all the files in .ssh\nchmod 644 ~/.ssh/*.pub # change public key permissions\nchmod 750 $HOME\n</code></pre> <p>https://www.tecmint.com/set-ssh-directory-permissions-in-linux/ https://superuser.com/questions/215504/permissions-on-private-key-in-ssh-folder</p>"},{"location":"lecture-notes/04/#external-references","title":"External References","text":"<ul> <li>https://linuxnatives.net/2019/how-to-create-good-ssh-keys</li> <li>https://www.digitalocean.com/community/tutorials/ssh-essentials-working-with-ssh-servers-clients-and-keys</li> <li>https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-on-ubuntu-20-04</li> <li>https://www.digitalocean.com/community/tutorials/how-to-configure-ssh-key-based-authentication-on-a-linux-server</li> <li>https://help.ubuntu.com/community/SSH/OpenSSH/Keys</li> </ul>"},{"location":"lecture-notes/05/","title":"Lecture 05 Notes","text":""},{"location":"lecture-notes/05/#locking-down-your-ssh","title":"Locking down your ssh","text":"<p>https://danaukes.com/notebook/ssh/disable-password-ssh/</p>"},{"location":"lecture-notes/05/#hostname","title":"Hostname","text":"<p>/etc/hostname</p>"},{"location":"lecture-notes/05/#ip-address-resolution","title":"IP Address resolution","text":"<p>/etc/hosts</p>"},{"location":"lecture-notes/05/#ssh-config-file","title":"SSH Config File","text":"<ul> <li>https://www.ssh.com/academy/ssh/config</li> <li>https://linuxize.com/post/using-the-ssh-config-file/</li> </ul>"},{"location":"lecture-notes/05/#netplan","title":"Netplan","text":"<ul> <li>https://askubuntu.com/questions/1122757/netplan-vs-networkmanager-on-ubuntu-18-04-and-above</li> </ul> <pre><code>sudo netplan try\n</code></pre> <p>other commands</p> <pre><code>sudo netplan generate\nsudo netplan apply\n</code></pre>"},{"location":"lecture-notes/05/#to-use-the-gui","title":"To use the GUI","text":"<pre><code>network:\n  version: 2\n  renderer: NetworkManager\n</code></pre>"},{"location":"lecture-notes/05/#access-point-mode","title":"Access Point Mode","text":"<pre><code>network: \n    version: 2 \n    ethernets: \n        ...\n    wifis: \n        renderer: NetworkManager \n        wlan0: \n            optional: true \n            access-points: \n                Turtlebot4: \n                    password: Turtlebot4 \n                    mode: ap \n                    #band: 5GHz\n                    #channel: 32\n                    band: 2.4GHz\n                    channel: 11 \n            dhcp4: true\n            addresses: [10.0.10.1/24]\n</code></pre> <p>See this article on available channels</p>"},{"location":"lecture-notes/05/#more-advanced-setup","title":"More advanced setup","text":"<ul> <li>https://danaukes.com/notebook/simple-netplan/</li> <li>https://netplan.io/examples</li> <li>https://netplan.io/reference/#properties-for-device-type-wifis%3A</li> <li>https://ubuntu.com/core/docs/networkmanager/networkmanager-and-netplan</li> </ul>"},{"location":"lecture-notes/05/#other-tools","title":"Other Tools","text":"<ul> <li><code>ifconfig</code>: simple tool for diagnostics and bringing an interface down or up</li> <li><code>ip address</code>: similar to ifconfig</li> <li><code>ip link</code>: list your network interfaces</li> <li><code>arp a</code>: list other recognized hosts on your network</li> <li><code>nmap</code>: really advanced tool for scanning hosts and ports</li> <li><code>nmcli</code>: diagnostic information about your connection</li> <li><code>nmcli d wifi</code> *</li> </ul>"},{"location":"lecture-notes/05/#other-helpful-ssh-stuff","title":"Other Helpful SSH Stuff","text":"<p>list all keys</p> <pre><code>ssh-add -l\n</code></pre> <p>remove all keys</p> <pre><code>ssh-add -D\n</code></pre>"},{"location":"lecture-notes/05/#add-passphrase-to-existing-key","title":"add passphrase to existing key","text":"<pre><code>ssh-keygen -p -f &lt;path-to-key&gt;\n</code></pre>"},{"location":"lecture-notes/05/#adding-users","title":"Adding users","text":"<pre><code>sudo adduser username\n</code></pre>"},{"location":"lecture-notes/05/#find-groups-associated-with-current-user","title":"Find groups associated with current user","text":"<pre><code>groups $USER\n</code></pre> <pre><code>sudo usermod -aG adm username\nsudo usermod -aG sudo username\n</code></pre>"},{"location":"lecture-notes/05/#remove-user","title":"Remove User","text":"<pre><code>sudo deluser --remove-home username\n</code></pre>"},{"location":"lecture-notes/05/#list-users-groups","title":"list users / groups","text":"<pre><code>getent passwd\ngetent group\n</code></pre>"},{"location":"lecture-notes/05/#see-who-is-logged-on","title":"See who is logged on","text":"<pre><code>users\nwho\n</code></pre>"},{"location":"lecture-notes/07-08/","title":"Lecture 07-08 Notes","text":"<ul> <li>ros2<ul> <li>general<ul> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-CLI-Tools/Configuring-ROS2-Environment.html</li> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-CLI-Tools/Understanding-ROS2-Nodes/Understanding-ROS2-Nodes.html</li> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-Client-Libraries/Creating-Your-First-ROS2-Package.html</li> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-CLI-Tools/Introducing-Turtlesim/Introducing-Turtlesim.html</li> <li>https://bookshelf.vitalsource.com/reader/books/9781000629811/epubcfi/6/16[%3Bvnd.vst.idref%3Dchapter2]!/4/2/6/68[comp02_11]/2</li> <li>https://ubuntu.com/tutorials/getting-started-with-ros-2#1-overview</li> </ul> </li> <li>services<ul> <li>https://answers.ros.org/question/302192/ros2-commandline-service-call/</li> </ul> </li> <li>Messages<ul> <li>https://roboticsbackend.com/ros2-create-custom-message/</li> </ul> </li> </ul> </li> <li>turtlebot info<ul> <li>https://turtlebot.github.io/turtlebot4-user-manual/overview/quick_start.html</li> <li>https://turtlebot.github.io/turtlebot4-user-manual/software/sensors.html#rplidar-a1m8</li> </ul> </li> <li>Create 3 Messages<ul> <li>https://iroboteducation.github.io/create3_docs/api/ros2/</li> <li>https://iroboteducation.github.io/create3_docs/api/safety/</li> <li>https://robotics.stackexchange.com/questions/22807/getting-a-head-start-with-irobot-create3-ros2</li> <li>https://github.com/iRobotEducation/irobot_create_msgs</li> <li>https://github.com/iRobotEducation/create3_examples</li> </ul> </li> <li>opencv<ul> <li>https://automaticaddison.com/getting-started-with-opencv-in-ros-2-galactic-python/</li> <li>https://answers.ros.org/question/383007/opencv-and-ros2/</li> <li>https://github.com/nightduck/ros2_opencv_demos</li> </ul> </li> </ul>"},{"location":"lecture-notes/11/","title":"Lecture 11 Notes","text":""},{"location":"lecture-notes/11/#list-packages","title":"List Packages","text":"<p>ros2 pkg list</p>"},{"location":"lecture-notes/11/#how-to-find-message-structure-and-location","title":"how to find message structure and location","text":"<p><code>/opt/ros/galactic</code></p>"},{"location":"lecture-notes/11/#qos-quality-of-service","title":"QOS: Quality of Service","text":"<pre><code>ros2 topic info --verbose /imu\n</code></pre> <p>https://docs.ros.org/en/rolling/How-To-Guides/Overriding-QoS-Policies-For-Recording-And-Playback.html https://design.ros2.org/articles/qos.html https://docs.ros.org/en/galactic/Concepts/About-Quality-of-Service-Settings.html</p>"},{"location":"lecture-notes/11/#override-qos-parameters","title":"Override QOS Parameters","text":"<pre><code>nano override.yaml\n</code></pre> <pre><code>/imu:\n  reliability: best_effort\n  durability: volatile\n  history: keep_all\n/color/preview/image:\n  reliability: best_effort\n  durability: volatile\n  history: keep_all\n</code></pre>"},{"location":"lecture-notes/11/#ros-bag","title":"Ros Bag","text":"<p>Way to record and play back data streams</p> <ul> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-CLI-Tools/Recording-And-Playing-Back-Data/Recording-And-Playing-Back-Data.html</li> <li>https://roboticsbackend.com/ros2-bag-save-and-replay-topic-data/</li> </ul>"},{"location":"lecture-notes/11/#recording","title":"Recording","text":"<pre><code>ros2 bag record -o ~/rosbag/camera /color/preview/image\nros2 bag record -o ~/rosbag/imu /imu --qos-profile-overrides-path ~/override.yaml\n</code></pre>"},{"location":"lecture-notes/11/#playing-back","title":"Playing Back","text":"<pre><code>ros2 bag play ~/rosbag/camera -r 1\nros2 bag play ~/rosbag/imu -r 1 --remap /imu:=/imu2 --qos-profile-overrides-path ~/override.yaml \n</code></pre> <p>https://answers.ros.org/question/328687/question-about-ros2-bag-play/</p> <p>Looping</p> <pre><code>ros2 bag play ~/rosbag/camera -r 1 -l\n</code></pre> <p>* message time is not shifted.</p>"},{"location":"lecture-notes/11/#python-and-qos","title":"Python and QOS","text":"<p>Need to Specify QOS parameter in subscription</p> <ul> <li>https://turtlebot.github.io/turtlebot4-user-manual/tutorials/first_node_python.html</li> <li>https://answers.ros.org/question/394442/new-publisher-discovered-on-topic-zedrightimage_rect_color-offering-incompatible-qos-no-messages-will-be-received-from-it-last-incompatible-policy/</li> </ul>"},{"location":"lecture-notes/13/","title":"Lecture 13 Notes","text":""},{"location":"lecture-notes/13/#updates","title":"Updates","text":"<pre><code>sudo apt remove yt-dlp\npip install yt-dlp\n</code></pre> <pre><code>conda create -n cv\nconda activate cv\nconda install -y jupyter matplotlib numpy scipy\nconda install -y -c conda-forge opencv\npip install yt-dlp\n</code></pre>"},{"location":"lecture-notes/13/#tutorials","title":"Tutorials","text":"<ul> <li>https://docs.opencv.org/3.4/d9/df8/tutorial_root.html</li> <li>Filtering: https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04</li> </ul>"},{"location":"lecture-notes/13/#finding-corners","title":"Finding Corners","text":"<ul> <li>https://docs.opencv.org/3.4/d4/d7d/tutorial_harris_detector.html</li> <li>https://stackoverflow.com/questions/7263621/how-to-find-corners-on-a-image-using-opencv</li> <li>https://docs.opencv.org/3.4/pattern.png</li> </ul>"},{"location":"lecture-notes/13/#keypoints","title":"KeyPoints","text":"<p>https://www.pythonpool.com/opencv-keypoint/ https://docs.opencv.org/3.4/d4/d5d/group__features2d__draw.html</p>"},{"location":"lecture-notes/13/#calibration","title":"Calibration","text":"<ul> <li>https://automaticaddison.com/how-to-perform-camera-calibration-using-opencv/</li> <li>https://www.geeksforgeeks.org/camera-calibration-with-python-opencv/</li> <li>https://docs.opencv.org/3.4/pattern.png</li> </ul>"},{"location":"lecture-notes/13/#background-removal","title":"Background removal","text":"<ul> <li>https://docs.opencv.org/4.x/d1/dc5/tutorial_background_subtraction.html</li> <li>https://www.freedomvc.com/index.php/2022/01/17/basic-background-remover-with-opencv/</li> <li>https://hostadvice.com/how-to/how-to-do-background-removal-in-a-video-using-opencv/</li> <li>https://pyimagesearch.com/2021/01/19/opencv-bitwise-and-or-xor-and-not/</li> <li>https://docs.opencv.org/3.4/db/df6/tutorial_erosion_dilatation.html</li> <li>https://docs.opencv.org/4.x/d0/d86/tutorial_py_image_arithmetics.html</li> </ul>"},{"location":"lecture-notes/13/#meanshift-camshift","title":"Meanshift &amp; Camshift","text":"<ul> <li>https://docs.opencv.org/3.4/d7/d00/tutorial_meanshift.html</li> </ul>"},{"location":"lecture-notes/13/#optical-flow","title":"Optical Flow","text":"<ul> <li>keypoints</li> </ul> <p>https://docs.opencv.org/3.4/db/d7f/tutorial_js_lucas_kanade.html https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html https://learnopencv.com/optical-flow-in-opencv/</p>"},{"location":"lecture-notes/13/#slam","title":"SLAM","text":"<ul> <li>https://turtlebot.github.io/turtlebot4-user-manual/software/slam.html</li> <li>https://github.com/turtlebot/turtlebot4</li> <li>https://github.com/SteveMacenski/slam_toolbox</li> </ul>"},{"location":"lecture-notes/14/","title":"Lecture 14 Notes","text":""},{"location":"lecture-notes/14/#external-resources","title":"External Resources","text":"<ul> <li>Turtlebot Package Information</li> <li>Driving your Turtlebot</li> <li>Oak D Camera ROS Repository</li> <li>Turtlebot Oak-D Information</li> </ul>"},{"location":"lecture-notes/14/#rviz","title":"RVIZ","text":"<pre><code>sudo apt install ros-galactic-rviz2\nrviz2\n</code></pre>"},{"location":"lecture-notes/14/#update-udev","title":"Update udev","text":"<p>Question: does /etc/udev/rules.d/80-movidius.rules exist?</p> <pre><code>echo 'SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"03e7\", MODE=\"0666\"' | sudo tee /etc/udev/rules.d/80-movidius.rules\nsudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger\n</code></pre>"},{"location":"lecture-notes/14/#try-to-run-default-oakd-launch-file","title":"Try to run default oakd launch file","text":""},{"location":"lecture-notes/14/#launch-files","title":"Launch Files","text":"<ul> <li>launch system</li> <li>https://docs.ros.org/en/galactic/Tutorials/Intermediate/Launch/Launch-Main.html</li> <li>https://roboticscasual.com/tutorial-ros2-launch-files-all-you-need-to-know/#launch-event-list</li> </ul>"},{"location":"lecture-notes/14/#find-launch-files","title":"Find Launch Files","text":"<pre><code>cd /opt/ros/galactic\nfind . -iname \"*launch*\"\n</code></pre>"},{"location":"lecture-notes/14/#kill-auto-started-node","title":"Kill Auto-Started node","text":"<pre><code>ros node list\nps -aux | grep rgb_stereo_node\nkill 1150\nkillall rgb_stereo_node\n</code></pre>"},{"location":"lecture-notes/14/#find-the-launch-file","title":"Find the Launch File","text":"<p>From here, we see that we are looking for <code>oakd.launch.py</code></p> <pre><code>cd /opt/ros/galactic/share/turtlebot4_bringup/launch\ncat oakd.launch.py \n</code></pre>"},{"location":"lecture-notes/14/#find-parameters","title":"Find Parameters","text":"<pre><code>ros2 launch turtlebot4_bringup oakd.launch.py -s\n</code></pre>"},{"location":"lecture-notes/14/#parameters","title":"Parameters","text":"<ul> <li><code>tf_prefix</code>: The name of the camera.</li> <li><code>publish_urdf</code>: Whether to publish the urdf</li> <li><code>colorResolution</code>: The resolution of the color camera</li> <li><code>useVideo</code>: Whether to publish a video of color image</li> <li><code>usePreview</code>: Whether to publish a preview of color image</li> <li><code>useDepth</code>: Whether to publish the depth image</li> <li><code>previewWidth</code>: Width of preview image</li> <li><code>previewHeight</code>: Height of preview image</li> </ul>"},{"location":"lecture-notes/14/#get-hidef-video-running","title":"Get hidef video running","text":"<pre><code>ros2 launch turtlebot4_bringup oakd.launch.py useVideo:=True\n</code></pre>"},{"location":"lecture-notes/14/#learn-about-your-topics","title":"Learn about your topics","text":"<pre><code>ros2 topic info --verbose /color/video/image\nros2 topic info --verbose /color/video/camera_info\nros2 topic echo /color/video/camera_info\n</code></pre>"},{"location":"lecture-notes/14/#learn-about-camera-info","title":"Learn about Camera-info","text":"<p>https://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/CameraInfo.html</p>"},{"location":"lecture-notes/15/","title":"Lecture 15 Notes","text":""},{"location":"lecture-notes/15/#create-a-new-package","title":"Create a new package","text":"<p>Why?  </p> <ul> <li>Good convention</li> <li>Python-only packages don't work with messages</li> </ul> <pre><code>ros2 pkg create --build-type ament_cmake dan_messages\n</code></pre>"},{"location":"lecture-notes/15/#create-a-new-message","title":"Create a new Message","text":"<pre><code>cd &lt;...&gt;/&lt;workspace_name&gt;/src/&lt;package_name&gt;\nmkdir msg\n## filename must start with a capital\nnano msg/MyPos.msg\n</code></pre> <pre><code>float64 x\nfloat64 y\n</code></pre>"},{"location":"lecture-notes/15/#dont-forget-dependencies","title":"Don't Forget Dependencies","text":"<p>...everywhere</p>"},{"location":"lecture-notes/15/#where-is-ros2-again","title":"Where is ROS2 again?","text":"<pre><code>which ros2\n</code></pre>"},{"location":"lecture-notes/15/#example","title":"Example","text":"<p>cat /opt/ros/galactic/share/geometry_msgs/msg/Polygon.msg</p>"},{"location":"lecture-notes/15/#modify-cmakelists","title":"Modify CMakeLists","text":"<ul> <li>Only possible if you make an <code>ament_cmake</code> type project</li> <li> <p>CMakeLists.txt is only generated for packages that are compiled by cmake.</p> <ul> <li><code>ament_cmake</code></li> <li><code>ament_cmake_python</code> (most likely similar to ROS 1 behavior)</li> </ul> </li> <li> <p>https://answers.ros.org/question/350084/define-custom-messages-in-python-package-ros2/</p> </li> </ul>"},{"location":"lecture-notes/15/#_1","title":"Lecture 15 Notes","text":"<pre><code>nano CMakeLists.txt\n</code></pre> <p>Paste the following above <code>ament_package()</code>:</p> <pre><code>find_package(geometry_msgs REQUIRED)\nfind_package(rosidl_default_generators REQUIRED)\n\nrosidl_generate_interfaces(${PROJECT_NAME}\n  \"msg/MyPos.msg\"\n  DEPENDENCIES geometry_msgs # Add packages that above messages depend on, in this case geometry_msgs for Sphere.msg\n)\n</code></pre>"},{"location":"lecture-notes/15/#fix-packagexml","title":"Fix Package.xml","text":"<p>just before <code>&lt;/package&gt;</code></p> <pre><code>  &lt;depend&gt;geometry_msgs&lt;/depend&gt;\n  &lt;build_depend&gt;rosidl_default_generators&lt;/build_depend&gt;\n  &lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;\n  &lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;\n</code></pre>"},{"location":"lecture-notes/15/#prep-the-package","title":"prep the package","text":"<pre><code>cd ~/dan_ws\nrosdep install -i --from-path src --rosdistro galactic -y\ncolcon build --packages-select dan_messages\n. install/local_setup.bash \n</code></pre>"},{"location":"lecture-notes/15/#my-error","title":"My Error","text":"<p>I named my package <code>mypos.msg</code> instead of <code>MyPos.msg</code>.  Ros2 has some strict naming conventions</p> <pre><code>Starting &gt;&gt;&gt; dan_messages\nStarting &gt;&gt;&gt; opencv_testing\nFinished &lt;&lt;&lt; opencv_testing [1.01s]                                                  \n--- stderr: dan_messages\nCMake Error at /opt/ros/galactic/share/rosidl_adapter/cmake/rosidl_adapt_interfaces.cmake:60 (message):\n\n....\n\n  rosidl_adapter.parser.InvalidResourceName: 'mypos' is an invalid message\n  name.  It should have the pattern '^[A-Z][A-Za-z0-9]*$'\n\n...\n\n---\nFailed   &lt;&lt;&lt; dan_messages [1.07s, exited with code 1]\n\nSummary: 1 package finished [1.27s]\n  1 package failed: dan_messages\n  1 package had stderr output: dan_messages\n</code></pre>"},{"location":"lecture-notes/15/#check","title":"Check","text":"<pre><code>ros2 interface show dan_messages/msg/MyPos\n</code></pre>"},{"location":"lecture-notes/15/#using-custom-messages","title":"Using custom messages","text":""},{"location":"lecture-notes/15/#simple-publisher","title":"Simple Publisher","text":"<p>from here</p> <pre><code>import rclpy\nfrom rclpy.node import Node\n\nfrom std_msgs.msg import String\n\n\nclass MinimalPublisher(Node):\n\n    def __init__(self):\n        super().__init__('minimal_publisher')\n        self.publisher_ = self.create_publisher(String, 'topic', 10)\n        timer_period = 0.5  # seconds\n        self.timer = self.create_timer(timer_period, self.timer_callback)\n        self.i = 0\n\n    def timer_callback(self):\n        msg = String()\n        msg.data = 'Hello World: %d' % self.i\n        self.publisher_.publish(msg)\n        self.get_logger().info('Publishing: \"%s\"' % msg.data)\n        self.i += 1\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    minimal_publisher = MinimalPublisher()\n\n    rclpy.spin(minimal_publisher)\n\n    # Destroy the node explicitly\n    # (optional - otherwise it will be done automatically\n    # when the garbage collector destroys the node object)\n    minimal_publisher.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"lecture-notes/15/#modifications","title":"Modifications","text":"<pre><code>#from std_msgs.msg import String\nfrom dan_messages.msg import MyPos\nimport random\n</code></pre> <pre><code>...\n#self.publisher_ = self.create_publisher(String, 'topic', 10)\nself.publisher_ = self.create_publisher(MyPos, 'topic', 10)\n...\n</code></pre> <pre><code>...\n#msg = String()\n#msg.data = 'Hello World: %d' % self.i\nmsg = MyPos()\nmsg.x = random.random()\nmsg.y = random.random()\n...\n</code></pre> <pre><code>...\n#self.get_logger().info('Publishing: \"%s\"' % msg.data)\nself.get_logger().info('Publishing: x:{}, y:{}'.format(msg.x,msg.y))\n...\n</code></pre>"},{"location":"lecture-notes/15/#other-to-do-list-items","title":"Other to-do-list items","text":""},{"location":"lecture-notes/15/#update-setuppy","title":"Update <code>setup.py</code>","text":"<pre><code>        'pos_publisher = opencv_testing.pos_publisher:main',\n</code></pre>"},{"location":"lecture-notes/15/#update-packagexml","title":"Update package.xml","text":"<p>add the following line to the list of dependencies:</p> <pre><code>&lt;depend&gt;dan_messages&lt;/depend&gt;\n</code></pre>"},{"location":"lecture-notes/15/#build","title":"Build","text":"<pre><code>cd ~/dan_ws\nrosdep install -i --from-path src --rosdistro galactic -y\ncolcon build\n. install/local_setup.bash \n</code></pre>"},{"location":"lecture-notes/15/#run","title":"Run","text":"<pre><code>ros2 run opencv_testing pos_publisher \n</code></pre> <p>in a second terminal, run</p> <pre><code>cd ~/dan_ws\n. install/local_setup.bash\nros2 topic echo /topic\n</code></pre>"},{"location":"lecture-notes/15/#resources","title":"Resources","text":"<ul> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-Client-Libraries/Custom-ROS2-Interfaces.html</li> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-Client-Libraries/Single-Package-Define-And-Use-Interface.html</li> </ul>"},{"location":"lecture-notes/19/","title":"Deep Learning &amp; Artificial Neural Networks","text":""},{"location":"lecture-notes/19/#deep-learning","title":"Deep Learning","text":"<ul> <li>aka \"Representation Learning\"</li> <li>\\(&gt;5\\) Layers is considered \"deep\"</li> <li>graph technologies with transformations among neurons</li> </ul>"},{"location":"lecture-notes/19/#caveat","title":"Caveat","text":"<ul> <li>This field is moving quickly.</li> </ul>"},{"location":"lecture-notes/19/#applications","title":"Applications","text":"<ul> <li>Natural Languge Processing (NLP)<ul> <li>Summarization</li> <li>Question Answering</li> <li>Paraphrase Identification</li> <li>Machine Translation</li> <li>Sentiment Analysis</li> </ul> </li> <li> <p>Visual Data Processing</p> <ul> <li>Image Classification</li> <li>Object Detection / Segmentation</li> <li>Video Processing</li> <li>Visual Datasets</li> </ul> </li> <li> <p>Speech and Audio Processing</p> <ul> <li>Speech Enhancement</li> <li>Speech Emotion Recognition</li> <li>Speech Recognition</li> </ul> </li> <li>Other<ul> <li>Social Network Analysis</li> <li>Information Retrieval</li> <li>Transportation Prediction</li> <li>Video Recommendation</li> <li>Autonomous Driving</li> <li>Biomedicine</li> <li>...</li> </ul> </li> </ul>"},{"location":"lecture-notes/19/#features","title":"Features","text":"<p>Typically the quality of machine learning suffers from the problem of GIGO</p> <p>Garbage In, Garbage Out</p>"},{"location":"lecture-notes/19/#_1","title":"Lecture 19 notes","text":"<p>\"Feature Engineering\" tries to address this by defining features of a particular kind of data that are easier to learn with.</p> <p>In Computer vision, many techniques have been proposed:</p> <ul> <li>Histogram of Oriented Gradients</li> <li>Scale Invariant Feature Transform</li> <li>Bag of Words</li> </ul> <p>Deep Learning performs feature extraction itself</p>"},{"location":"lecture-notes/19/#history","title":"History","text":"<ul> <li>Aristotole proposed \"associationism\"</li> <li>MCP: McCulloch &amp; Pitts, 1943<ul> <li>used \"threshold logic\" to mimic thought, not learning</li> </ul> </li> <li>Hebbian theory: Hebb, 1949</li> <li>\"Perceptron\": Rosenblatt, 1958</li> <li>Back-propogoation: Werbos, 1974<ul> <li>using error to train deep learning models</li> </ul> </li> <li>Neocogitron: Fukushima, 1980</li> <li>Botzmann Machine: Ackley, Hinton &amp; Sejnowski, 1985</li> <li>Restricted Botzmann Machine (RBM): Smolensky, 1986</li> <li>Recurrent Neural Network: Jordan, 1986</li> <li>Autoencoders:<ul> <li>Rumelhart, Hinton &amp; Williams, 1986</li> <li>Ballard, 1987</li> </ul> </li> <li>LeNet: LeCun, 1990</li> <li>Deep Belief Networks: Hinton, 2006<ul> <li>Layer-wise training &amp; Deep Learning</li> </ul> </li> <li>Deep Boltzmann Machine: Salakhutdinov &amp; Hinton, 2009</li> <li>AlexNet: Krizhevsky, Sutskever, &amp; Hinton, 2012</li> <li>AlphaGo: 2017</li> </ul>"},{"location":"lecture-notes/19/#types-of-networks","title":"Types of Networks","text":""},{"location":"lecture-notes/19/#recursive-neural-networks-rvnn","title":"Recursive Neural Networks (RvNN)","text":"<ul> <li> <p>Predictions related to hierarchical structures</p> </li> <li> <p>classify outputs with vectors</p> </li> <li>Inspired by Recursive Autoassociative Memory (RAAM), 1996<ul> <li>trees/graphs</li> </ul> </li> <li>Can be used in NLP to build \"syntactic trees\"</li> </ul>"},{"location":"lecture-notes/19/#recurrent-neural-networks-rnn","title":"Recurrent Neural Networks (RNN)","text":"<ul> <li>Good for sequential information like NLP, speech processing</li> <li>Uses \"short-term memory units\" that include input layers, hidden state layers, and output layers<ul> <li>Sensitive to derivative terms and vanishing/exploding gradients</li> </ul> </li> <li>Long Short-Term Memory (LSTM) provides memory blocks in recurrent connections</li> <li>Residual connections can alleviate vanishing gradients as well</li> </ul>"},{"location":"lecture-notes/19/#convolutional-neural-networks-1995","title":"Convolutional Neural Networks: 1995","text":"<ul> <li>Applications<ul> <li>Computer Vision</li> <li>NLP &amp; Speech Propcessing</li> </ul> </li> <li>Advantages:<ul> <li>sparse interactions (in contrast to fully connected networks)</li> <li>equivalent representations</li> <li>shared parameters</li> </ul> </li> <li>\"pattern recognition\"<ul> <li>Sensitive to parts / aspects of a scene</li> <li>inspired by and similar to how the visual cortex works in humans and animals</li> </ul> </li> </ul>"},{"location":"lecture-notes/19/#cnn-process","title":"CNN Process","text":"<ol> <li>convolution involves concept of \"kernel\", similar to linear filters.</li> <li>activation function: nonlinear function is applied to the output to amplify signal.</li> <li>pooling (average or max) downsamples and reduces paramaters and controls overfitting.</li> <li>final layer: softmax or svm to classify and provide probablilty.</li> </ol>"},{"location":"lecture-notes/19/#convolution-layer","title":"Convolution Layer","text":"<p>{style=\"width=100%;max-height:4in;\"}</p>"},{"location":"lecture-notes/19/#activation","title":"Activation","text":"<p>{style=\"width=100%;max-height:4in;\"}</p>"},{"location":"lecture-notes/19/#deep-generative-networks","title":"Deep Generative Networks","text":"<p>{style=\"width=100%;max-height:4in;\"}</p> <ul> <li>Deep Belief Networks (DBN)</li> <li>Deep Boltzmann Machine (DBM)</li> <li>Generative Adversarial Network (GAN)</li> <li>Variational Autoencoder (VAE)</li> </ul>"},{"location":"lecture-notes/19/#types-of-learning","title":"Types of Learning","text":"<ul> <li>Supervised Deep Learning</li> <li>Unsupervised Deep Learning<ul> <li>Using generative techniques to learn and bin new features</li> </ul> </li> <li>Transfer Learning<ul> <li>Pretrain on one dataset</li> <li>Fine-tune and train feature extractors with your own smaller dataset</li> </ul> </li> <li>Semi-Supervised Deep Learning<ul> <li>Training occurs with some labeled and some unlabeled data</li> </ul> </li> <li>Online Learning<ul> <li>Stochastic Gradient Descent approaches: samples update model parameters with known labels</li> <li>Achievements have historically been more limited.</li> <li>Sometimes the underlying dynamics of time-varying samples changes.</li> </ul> </li> </ul>"},{"location":"lecture-notes/19/#optimization","title":"Optimization","text":"<ul> <li>Training a DNN is an optimization process</li> <li>Stochastic Gradient Descent is most widely used technique</li> <li>\"Learning Rate\" controls the update speed with a stability/time tradeoff.<ul> <li>Concept of \"momentum\" introduced to stabilize and balance this tradeoff.</li> <li>Decay hyperparameters can be added: weight decay, learning rate decay.</li> <li>Adaptive learning rates.  Many techniques</li> </ul> </li> </ul>"},{"location":"lecture-notes/19/#models","title":"Models","text":"<ul> <li>You can share the results of training a model</li> <li>Because it's layer-based, you can keep some training and re-train selected layers</li> </ul>"},{"location":"lecture-notes/19/#datasets","title":"Datasets","text":"<ul> <li>MNIST</li> <li>CIFAR</li> <li>ImageNet</li> </ul> <p>Needs: Image/labels, audio/transcript</p>"},{"location":"lecture-notes/19/#toolkits","title":"Toolkits","text":"<ul> <li>PyTorch</li> <li>TensorFlow</li> <li>Theano</li> <li>DeepLearning4j</li> <li>Caffe</li> <li>MXNet</li> <li>CNTK</li> <li>...</li> </ul>"},{"location":"lecture-notes/19/#computer-vision-models","title":"Computer Vision Models","text":""},{"location":"lecture-notes/19/#classification","title":"Classification","text":"<ul> <li>Lenet</li> <li>AlexNet</li> <li>VGGNet</li> <li>GoogleNet</li> <li>ResNet</li> <li>ResNeXT</li> </ul>"},{"location":"lecture-notes/19/#detection-segmentation","title":"Detection / Segmentation","text":"<ul> <li>R-CNN: Region-based CNN</li> <li>YOLO</li> <li>Faster R-CNN</li> <li>R-FCNs</li> <li>SSD: Single Shot MultiBox Detector</li> </ul>"},{"location":"lecture-notes/19/#image-class-performance","title":"image-class-performance","text":"<p>{style=\"width=100%;max-height:4in;\"}</p>"},{"location":"lecture-notes/19/#references-and-resources","title":"References and Resources","text":"<ul> <li>Pouyanfar, S., Sadiq, S., Yan, Y., Tian, H., Tao, Y., Reyes, M. P., Shyu, M.-L., Chen, S.-C., &amp; Iyengar, S. S. (2019). A Survey on Deep Learning: Algorithms, Techniques, and Applications. ACM Computing Surveys, 51(5), 1\u201336. https://doi.org/10.1145/3234150</li> <li>Sze, V., Chen, Y.-H., Yang, T.-J., &amp; Emer, J. S. (2017). Efficient Processing of Deep Neural Networks: A Tutorial and Survey. Proceedings of the IEEE, 105(12), 2295\u20132329. https://doi.org/10.1109/JPROC.2017.2761740</li> <li>Voulodimos, A., Doulamis, N., Doulamis, A., &amp; Protopapadakis, E. (2018). Deep Learning for Computer Vision: A Brief Review. Computational Intelligence and Neuroscience, 2018, 1\u201313. https://doi.org/10.1155/2018/7068349</li> </ul>"},{"location":"lecture-notes/19/#misc","title":"Misc","text":""},{"location":"lecture-notes/19/#backpropogation","title":"Backpropogation","text":"<p>{style=\"width=100%;max-height:4in;\"}</p>"},{"location":"lecture-notes/20/","title":"Lecture 20 Notes","text":""},{"location":"lecture-notes/20/#external-resources","title":"External resources","text":"<ul> <li>https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/5-multilayer-convolutions</li> <li>https://www.kaggle.com/code/alpaca0984/dog-vs-cat-with-pytorch</li> <li>https://www.kaggle.com/competitions/dogs-vs-cats-redux-kernels-edition/data</li> </ul>"},{"location":"lecture-notes/20/#install-prerequisites","title":"Install Prerequisites","text":"<pre><code>conda create -n ai\nconda activate ai\nconda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n\n#conda install pytorch torchvision torchaudio cpuonly -c pytorch\nconda install jupyterlab ipywidgets\nconda install matplotlib\nconda install torchinfo\nconda install pandas\nconda install scikit-learn\nconda install scikit-learn-intelex\nconda install pyyaml\npip install opencv-python\npip install wget\n</code></pre>"},{"location":"lecture-notes/20/#python-code","title":"Python Code","text":"<pre><code>import wget\nwget.download(https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/computer-vision-pytorch/pytorchcv.py,'./')\n</code></pre> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nimport torchvision.transforms\nfrom torchvision import datasets\nfrom torchvision.io import read_image\nfrom torchinfo import summary\n\nimport pytorchcv\nfrom pytorchcv import load_mnist, train, plot_results, plot_convolution, display_dataset\n\nimport numpy\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image\n\nimport os\nimport glob\nimport wget\nimport zipfile\n\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>if not os.path.exists('data/kagglecatsanddogs_5340.zip'):\n    wget.download('https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip','data')\n</code></pre> <pre><code>if not os.path.exists('/home/danaukes/projects/project_robot-deployment/code/nn/data/PetImages'):\n    with zipfile.ZipFile('/home/danaukes/projects/project_robot-deployment/code/nn/data/kagglecatsanddogs_5340.zip', 'r') as zip_ref:\n        zip_ref.extractall('/home/danaukes/projects/project_robot-deployment/code/nn/data')    \n</code></pre> <pre><code>class CustomImageDataset(Dataset):\n    def __init__(self, image_paths,labels, transforms=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        if transforms is not None:\n            self.transforms = transforms[:]\n        else:\n            self.transforms = []\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, ii):\n        img_path = self.image_paths[ii]\n        image = read_image(img_path)\n\n        image = torch.tensor(image,dtype = torch.float32)\n\n        for item in self.transforms:\n            image = item(image)\n\n        return (image, self.labels[ii])\n</code></pre> <pre><code>labels_map = {0: \"cat\",1: \"dog\"}\n</code></pre> <pre><code>cats_path = \"/home/danaukes/projects/project_robot-deployment/code/nn/data/PetImages/Cat/\"\ndogs_path = \"/home/danaukes/projects/project_robot-deployment/code/nn/data/PetImages/Dog/\"\n\ncat_file_list = glob.glob(cats_path+\"*.jpg\")\ncat_paths = numpy.array(cat_file_list)\ncat_labels = numpy.zeros(cat_paths.shape,dtype=numpy.uint8)\n\ndog_file_list = glob.glob(dogs_path+\"*.jpg\")\ndog_paths = numpy.array(dog_file_list)\ndog_labels = numpy.ones(dog_paths.shape,dtype=numpy.uint8)\n\nlabels = numpy.r_[cat_labels,dog_labels]\n\npaths = numpy.r_[cat_paths,dog_paths]\nlabels = numpy.r_[cat_labels,dog_labels]\ngood_paths = []\ngood_labels = []\n</code></pre> <p>https://discuss.pytorch.org/t/corrupt-jpeg-data-24-extraneous-bytes-before-marker-0xd9/101292</p> <pre><code>for path,label in zip(paths,labels):\n    try:\n        im = read_image(path)\n        if im is not None:\n            if im.shape[0]==3:\n                good_paths.append(path)\n                good_labels.append(label)\n        else:\n            print(path+\"is none\")\n    except Exception as ex:\n        pass\n        print(path)\n        os.remove(path)\n</code></pre> <pre><code>Corrupt JPEG data: 237 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 1157 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nWarning: unknown JFIF revision number 0.00\nCorrupt JPEG data: 1405 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 157 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 399 extraneous bytes before marker 0xd9\n</code></pre> <pre><code>transforms = [torchvision.transforms.Resize(size=(32,32))]\n\n#mean  = [167.12461853027344, 139.81251525878906, 113.34357452392578]\n#std = [39.6484489440918, 48.59019470214844, 64.63758087158203]\n\n#mean  = [.5, .5, .5]\n#std  = [.5, .5, .5]\n\n#transforms = [torchvision.transforms.Resize(size=(32,32)),torchvision.transforms.Normalize(mean,std)] \n</code></pre> <pre><code>dataset=CustomImageDataset(good_paths,good_labels,transforms=transforms)\n</code></pre> <pre><code>l = len(dataset)\nn = int(l*.9)\n\ntrain_ds, test_ds = torch.utils.data.random_split(dataset,[n,l-n])\n</code></pre> <pre><code>trainloader = DataLoader(train_ds, batch_size=14, shuffle=True)\ntestloader = DataLoader(test_ds, batch_size=14, shuffle=True)\n</code></pre> <pre><code>classes = ('cat','dog')\n</code></pre> <pre><code>train_ds[0]\n</code></pre> <pre><code>/tmp/ipykernel_30248/3393794065.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  image = torch.tensor(image,dtype = torch.float32)\n\n\n\n\n\n(tensor([[[136.0000, 142.6499, 148.5103,  ...,  56.4585,  76.7817,  43.8335],\n          [142.2124, 138.6650, 147.7021,  ...,  33.9111,  87.9106,  67.8926],\n          [ 59.8989,  28.8286,  29.2305,  ...,  47.0356,  49.7515,  59.3506],\n          ...,\n          [108.1294, 128.6304,  90.0967,  ..., 226.0000, 222.5615, 144.9116],\n          [ 84.8135, 134.0293,  65.8623,  ..., 225.5156, 219.0312, 210.2158],\n          [108.4150, 142.8164,  87.7817,  ..., 224.0000, 220.0156, 117.5742]],\n\n         [[163.0000, 170.9624, 176.8228,  ...,  71.4585,  90.4536,  55.4897],\n          [170.8623, 167.5210, 179.5771,  ...,  42.9111, 101.9106,  81.5957],\n          [ 64.3730,  39.7979,  35.1055,  ...,  57.0356,  61.7515,  72.1528],\n          ...,\n          [101.8325, 129.9585,  95.3579,  ..., 191.0000, 188.5615, 119.2085],\n          [ 84.7949, 132.7168,  66.2227,  ..., 190.5156, 185.0312, 180.6846],\n          [112.5796, 145.6753,  95.1567,  ..., 189.0000, 185.3218,  99.0210]],\n\n         [[181.3125, 188.6187, 193.1665,  ...,  51.1567,  73.7817,  46.1772],\n          [179.8188, 181.4619, 192.6084,  ...,  38.9424,  78.6987,  59.4863],\n          [ 61.2324,  41.7979,  41.3867,  ...,  47.2119,  47.7515,  55.7461],\n          ...,\n          [ 75.8013,  90.0220,  54.1548,  ..., 101.0000,  99.5615,  67.3179],\n          [ 58.8042,  87.1753,  45.2852,  ..., 100.5156,  98.0312, 100.6626],\n          [ 83.6016, 112.3413,  59.5005,  ...,  99.0000, 100.5469,  47.4502]]]),\n 1)\n</code></pre> <pre><code>display_dataset(train_ds,classes=classes)\n</code></pre> <pre><code>/tmp/ipykernel_30248/3393794065.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  image = torch.tensor(image,dtype = torch.float32)\n</code></pre> <p></p> <pre><code>class LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.conv3 = nn.Conv2d(16,120,5)\n        self.flat = nn.Flatten()\n        self.fc1 = nn.Linear(120,64)\n        self.fc2 = nn.Linear(64,2)\n\n    def forward(self, x):\n        x = self.pool(nn.functional.relu(self.conv1(x)))\n        x = self.pool(nn.functional.relu(self.conv2(x)))\n        x = nn.functional.relu(self.conv3(x))\n        x = self.flat(x)\n        x = nn.functional.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nnet = LeNet()\n\nsummary(net,input_size=(1,3,32,32))\n</code></pre> <pre><code>==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nLeNet                                    [1, 2]                    --\n\u251c\u2500Conv2d: 1-1                            [1, 6, 28, 28]            456\n\u251c\u2500MaxPool2d: 1-2                         [1, 6, 14, 14]            --\n\u251c\u2500Conv2d: 1-3                            [1, 16, 10, 10]           2,416\n\u251c\u2500MaxPool2d: 1-4                         [1, 16, 5, 5]             --\n\u251c\u2500Conv2d: 1-5                            [1, 120, 1, 1]            48,120\n\u251c\u2500Flatten: 1-6                           [1, 120]                  --\n\u251c\u2500Linear: 1-7                            [1, 64]                   7,744\n\u251c\u2500Linear: 1-8                            [1, 2]                    130\n==========================================================================================\nTotal params: 58,866\nTrainable params: 58,866\nNon-trainable params: 0\nTotal mult-adds (M): 0.66\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.05\nParams size (MB): 0.24\nEstimated Total Size (MB): 0.30\n==========================================================================================\n</code></pre> <pre><code>opt = torch.optim.SGD(net.parameters(),lr=0.001,momentum=0.9)\nloss_fn = nn.CrossEntropyLoss()\n</code></pre> <pre><code>epochs = 8\nretrain = True\nif retrain:\n    hist = train(net, trainloader, testloader, epochs=epochs, optimizer=opt, loss_fn=loss_fn)\n    torch.save(net,'lenet2.pth')\nnet = torch.load('lenet2.pth')\n</code></pre> <pre><code>/tmp/ipykernel_30248/3393794065.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  image = torch.tensor(image,dtype = torch.float32)\nCorrupt JPEG data: 1157 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 1405 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 237 extraneous bytes before marker 0xd9\nWarning: unknown JFIF revision number 0.00\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 399 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 157 extraneous bytes before marker 0xd9\n\n\nEpoch  0, Train acc=0.521, Val acc=0.508, Train loss=0.050, Val loss=0.050\n\n\nCorrupt JPEG data: 1405 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 237 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 1157 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\nWarning: unknown JFIF revision number 0.00\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 399 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 157 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\n\n\nEpoch  1, Train acc=0.531, Val acc=0.544, Train loss=0.049, Val loss=0.049\n\n\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\nWarning: unknown JFIF revision number 0.00\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 1405 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\n</code></pre> <pre><code>all_correct = []\nnet.eval()\n\nfor inputs, target in testloader:\n    output = net(inputs)\n    _, pred = output.max(1)\n    correct = (target==output.argmax(1))\n    all_correct.extend(correct)\n\npercentage_right = float(sum(all_correct)/len(all_correct))*100\nprint(\"{0:2.2f}% right\".format(percentage_right))\n\n</code></pre> <pre><code>/tmp/ipykernel_52623/3393794065.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  image = torch.tensor(image,dtype = torch.float32)\n\n\n69.80% right\n</code></pre> <pre><code>l = len(test_ds)\nn = 10\nmytest,dummy = torch.utils.data.random_split(test_ds,[n,l-n])\nfinalloader = DataLoader(mytest, batch_size=n, shuffle=False)\n</code></pre> <pre><code>all_correct = []\nnet.eval()\n\nfor inputs, target in finalloader:\n    output = net(inputs)\n    _, pred = output.max(1)\n    correct = (target==output.argmax(1))\n    all_correct.extend(correct)\n\npercentage_right = float(sum(all_correct)/len(all_correct))*100\nprint(\"{0:2.2f}% right\".format(percentage_right))\n</code></pre> <pre><code>60.00% right\n\n\n/tmp/ipykernel_52623/3393794065.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  image = torch.tensor(image,dtype = torch.float32)\n</code></pre> <pre><code>fig,ax = plt.subplots(1,n,figsize=(33,10),dpi=50)\n\nphoto_indices = numpy.array(test_ds.indices)[mytest.indices]\n\nfor ii,(myin1,myten1,myout1,photo_index) in enumerate(zip(target,inputs,output,photo_indices)):\n    label = classes[myout1.argmax(0)] + ' ('+classes[myin1]+')'\n\n    im = Image.open(dataset.image_paths[photo_index])\n\n    ax[ii].imshow(im)\n    ax[ii].set_title(label)\n    ax[ii].axis('off')\n\n\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"lecture-notes/22/","title":"Lecture 22 Notes","text":"<ul> <li>Return Turtlebots Wednesday</li> <li>last deadline is next Wed with approval.</li> <li>final report due next wed.</li> </ul>"},{"location":"tutorials/parts/","title":"Parts","text":""},{"location":"tutorials/parts/#optional-parts","title":"Optional Parts","text":"<ul> <li>hex keys<ul> <li>1.5mm</li> <li>2mm</li> </ul> </li> <li>wifi dongle</li> <li>2.4Ghz / 5Ghz Wifi Access Point</li> <li>ethernet patch cable</li> </ul>"},{"location":"tutorials/vscode-info/","title":"VSCode Info","text":"<p>please see this page for vscode information</p>"},{"location":"tutorials/jupyter-and-python/grab-data/","title":"Grab Data","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import qos_profile_sensor_data\n\nfrom sensor_msgs.msg import Imu\n\nimport numpy\nimport yaml\n\n\nclass IMUDataLogger(Node):\n    labels = ['ax','ay','az']\n\n    def __init__(self):\n        super().__init__('imu_data_logger')\n        self.subscription = self.create_subscription(Imu,'/imu',self.listener_callback,qos_profile_sensor_data)\n        self.subscription  \n        self.t0 = None\n        self.history = None\n        self.fig  = None\n\n    def listener_callback(self, msg):\n        self.get_logger().info(\"received message\")\n\n        current_time = msg.header.stamp.sec+msg.header.stamp.nanosec*1e-9\n\n\n        if not self.t0:\n            self.t0 = current_time\n\n        t = current_time-self.t0\n\n        print(t)\n\n        if self.history is None:\n            self.history = numpy.array([[t, msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z]])\n        else:\n            self.history = numpy.concatenate([self.history,numpy.array([[t, msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z]])],0)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    imu_data_logger = IMUDataLogger()\n\n    try:\n        rclpy.spin(imu_data_logger)\n#    except SystemExit:\n#        print('exiting')\n    except KeyboardInterrupt:\n        mydata = imu_data_logger.history.tolist()\n        mydict = {'data':mydata,}\n        with open('file.yaml','w') as f:\n            yaml.dump(mydict,f)\n\n    imu_data_logger.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n\n</code></pre>"},{"location":"tutorials/jupyter-and-python/plot_imu_data/","title":"Plot IMU Data in ROS","text":"<pre><code># Derived from here: https://answers.ros.org/question/346650/matplotlib-dynamic-plot-in-ros2-callback/\n# Help here: https://stackoverflow.com/questions/35145555/python-real-time-plotting-ros-data\n# https://github.com/ros2/ros2/issues/509\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import qos_profile_sensor_data\n\nfrom sensor_msgs.msg import Imu\n\nimport matplotlib.pyplot as plt \nfrom matplotlib.animation import FuncAnimation\nimport numpy\n\nMEMORY = 50\n\nmy_frame  = numpy.zeros((MEMORY,2))\n\nclass IMUDataPlotter(Node):\n    labels = ['ax','ay','']\n\n    def __init__(self):\n        super().__init__('imu_data_plotter')\n        self.subscription = self.create_subscription(Imu,'/imu',self.listener_callback,qos_profile_sensor_data)\n        self.subscription  \n        self.t0 = None\n        self.history = None\n        self.fig  = None\n\n    def listener_callback(self, msg):\n        self.get_logger().info(\"received message\")\n\n        current_time = msg.header.stamp.sec+msg.header.stamp.nanosec*1e-9\n\n\n        if not self.t0:\n            self.t0 = current_time\n\n        t = current_time-self.t0\n\n        print(t)\n\n        if self.history is None:\n            self.history = numpy.zeros((MEMORY,4))\n\n        self.history[:-1,:] = self.history[1:,:]\n        self.history[-1,:] = numpy.array([t, msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z])\n        self.update(1)\n\n    def update(self,i):\n\n        if self.fig is None:\n            first = True\n            self.fig = plt.figure()\n            self.ax = plt.subplot(111)\n        else:\n            first = False\n        self.ax.cla()\n        self.ax.plot(self.history[:,0],self.history[:,1])\n        self.ax.legend(['ax'],loc = \"upper left\")\n        self.ax.set_ylim(self.history[:,1].min(),self.history[:,1].max())    # plot memory\n        self.fig.canvas.draw()\n        if first:\n            plt.ion()\n            plt.show()\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    imu_data_plotter = IMUDataPlotter()\n\n    plt.ion()\n    plt.show()\n\n    rclpy.spin(imu_data_plotter)\n\n    imu_data_plotter.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n\n</code></pre>"},{"location":"tutorials/jupyter-and-python/fft/fft/","title":"Plotting FFTs","text":"<p>Start by setting matplotlib to plot inline</p> <pre><code>%matplotlib inline\n</code></pre> <p>import packages</p> <pre><code>import numpy\nimport matplotlib.pyplot as plt\nimport yaml\nfrom math import pi\nimport scipy.fft\n</code></pre> <p>load data and convert to a numpy array</p> <pre><code>with open('/home/danaukes/dan_ws/file.yaml','r') as f:\n    mydictionary = yaml.load(f,Loader=yaml.FullLoader)\n\ndata = mydictionary['data']\ndata = numpy.array(data)\n</code></pre> <p>plot the raw x-axis acceleration data</p> <pre><code>plt.plot(data[:,0],data[:,1])\nplt.xlabel('time (s)')\nplt.ylabel('x acceleration')\n</code></pre> <pre><code>Text(0, 0.5, 'x acceleration')\n</code></pre> <p></p> <p>Use the FFT to convert from the time domain to the frequency domain</p> <pre><code>z = scipy.fft.fft(data[:,1])\n</code></pre> <pre><code>plt.plot(z)\n</code></pre> <pre><code>/usr/lib/python3/dist-packages/matplotlib/cbook/__init__.py:1390: ComplexWarning: Casting complex values to real discards the imaginary part\n  return np.asarray(x, float)\n\n\n\n\n\n[&lt;matplotlib.lines.Line2D at 0x7efc17e10130&gt;]\n</code></pre> <p></p> <pre><code>dt = numpy.mean(data[1:,0]-data[:-1,0])\ndt\n</code></pre> <pre><code>0.016086905679075777\n</code></pre> <pre><code>l = len(z)\nl = z.shape[0]\nprint('l = {0}'.format(l))\nw = 2*pi/dt*numpy.arange(1,l+1)/l\nw\n</code></pre> <pre><code>l = 1157\n\n\n\n\n\narray([3.37577890e-01, 6.75155779e-01, 1.01273367e+00, ...,\n       3.89902463e+02, 3.90240040e+02, 3.90577618e+02])\n</code></pre> <p>Plot all the data.</p> <pre><code>fig = plt.figure()\nplt.semilogx(w,20*numpy.log10(numpy.abs(numpy.real(z))))\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x7efc180fcdc0&gt;]\n</code></pre> <p></p> <p>Plot only the frequencies below the Nyquist frequency</p> <pre><code>fig = plt.figure()\nplt.semilogx(w[:int(l/2)],20*numpy.log10(numpy.abs(numpy.real(z[:int(l/2)]))))\nplt.xlabel('w (rad/s)')\nplt.ylabel('dB')\n</code></pre> <pre><code>Text(0, 0.5, 'dB')\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"tutorials/jupyter-and-python/filtering-example/filtering-example/","title":"Simple Filtering Example","text":"<pre><code>import numpy as np\nimport scipy as sp\n!pip install pandas\nimport pandas as pd\n</code></pre> <pre><code>Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (1.5.3)\nRequirement already satisfied: numpy&gt;=1.20.3 in ./.local/lib/python3.8/site-packages (from pandas) (1.24.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in ./.local/lib/python3.8/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in ./.local/lib/python3.8/site-packages (from pandas) (2022.7.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/lib/python3/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas) (1.14.0)\n</code></pre> <pre><code>%matplotlib inline\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n</code></pre> <pre><code>nasdaq_df = pd.read_csv(\n    'https://github.com/ipython-books/'\n    'cookbook-2nd-data/blob/master/'\n    'nasdaq.csv?raw=true',\n    index_col='Date',\n    parse_dates=['Date'])\n\nnasdaq_df.head()\n</code></pre> Open High Low Close Adj Close Volume Date 1990-01-02 452.899994 459.299988 452.700012 459.299988 459.299988 110720000 1990-01-03 461.100006 461.600006 460.000000 460.899994 460.899994 152660000 1990-01-04 460.399994 460.799988 456.899994 459.399994 459.399994 147950000 1990-01-05 457.899994 459.399994 457.799988 458.200012 458.200012 137230000 1990-01-08 457.100006 458.700012 456.500000 458.700012 458.700012 115500000 <pre><code>date = nasdaq_df.index\nnasdaq = nasdaq_df['Close']\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(6, 4))\nnasdaq.plot(ax=ax, lw=1)\n</code></pre> <pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fea4e16b5b0&gt;\n</code></pre> <pre><code>import scipy.signal as sg\n# We get a triangular window with 60 samples.\nh = sg.get_window('triang', 60)\n# We convolve the signal with this window.\nfil = sg.convolve(nasdaq, h / h.sum())\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n# We plot the original signal...\nnasdaq.plot(ax=ax, lw=3)\n# ... and the filtered signal.\nax.plot_date(date, fil[:len(nasdaq)],\n             '-w', lw=2)\n\n</code></pre> <pre><code>/usr/lib/python3/dist-packages/matplotlib/cbook/__init__.py:1402: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n  ndim = x[:, None].ndim\n/usr/lib/python3/dist-packages/matplotlib/axes/_base.py:276: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n  x = x[:, np.newaxis]\n\n\n\n\n\n[&lt;matplotlib.lines.Line2D at 0x7fea42cea760&gt;]\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(6, 4))\nnasdaq.plot(ax=ax, lw=3)\n# We create a 4-th order Butterworth low-pass filter.\nb, a = sg.butter(4, 2. / 365,analog=False)\n# We apply this filter to the signal.\nax.plot_date(date, sg.filtfilt(b, a, nasdaq),\n             '-w', lw=2)\n</code></pre> <pre><code>/usr/lib/python3/dist-packages/matplotlib/cbook/__init__.py:1402: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n  ndim = x[:, None].ndim\n/usr/lib/python3/dist-packages/matplotlib/axes/_base.py:276: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n  x = x[:, np.newaxis]\n\n\n\n\n\n[&lt;matplotlib.lines.Line2D at 0x7fea40f40b50&gt;]\n</code></pre> <pre><code>b\n</code></pre> <pre><code>array([5.36664515e-09, 2.14665806e-08, 3.21998709e-08, 2.14665806e-08,\n       5.36664515e-09])\n</code></pre> <pre><code>a\n</code></pre> <pre><code>array([ 1.        , -3.95501738,  5.8660611 , -3.86705685,  0.95601322])\n</code></pre> <pre><code>h\n</code></pre> <pre><code>array([0.03225806, 0.06451613, 0.09677419, 0.12903226, 0.16129032,\n       0.19354839, 0.22580645, 0.25806452, 0.29032258, 0.32258065,\n       0.35483871, 0.38709677, 0.41935484, 0.4516129 , 0.48387097,\n       0.51612903, 0.5483871 , 0.58064516, 0.61290323, 0.64516129,\n       0.67741935, 0.70967742, 0.74193548, 0.77419355, 0.80645161,\n       0.83870968, 0.87096774, 0.90322581, 0.93548387, 0.96774194,\n       1.        , 0.96774194, 0.93548387, 0.90322581, 0.87096774,\n       0.83870968, 0.80645161, 0.77419355, 0.74193548, 0.70967742,\n       0.67741935, 0.64516129, 0.61290323, 0.58064516, 0.5483871 ,\n       0.51612903, 0.48387097, 0.4516129 , 0.41935484, 0.38709677,\n       0.35483871, 0.32258065, 0.29032258, 0.25806452, 0.22580645,\n       0.19354839, 0.16129032, 0.12903226, 0.09677419, 0.06451613])\n</code></pre> <pre><code>plt.plot(h)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x7fea40cc1370&gt;]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/jupyter-and-python/simple-plot-example/simple-plot-example/","title":"Simple Plotting Example","text":"<pre><code>%matplotlib inline\n</code></pre> <p>!pip install matplotlib numpy scipy</p> <pre><code>import matplotlib\nimport numpy\nimport scipy\n</code></pre> <pre><code>x = numpy.array([0,1,2,3,4,5,6,7])\nx\n</code></pre> <pre><code>array([0, 1, 2, 3, 4, 5, 6, 7])\n</code></pre> <pre><code>y = 2*x\ny\n</code></pre> <pre><code>array([ 0,  2,  4,  6,  8, 10, 12, 14])\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n#figure = plt.figure(figsize=(3,2))\nfigure = plt.figure(dpi=50)\nax = plt.subplot(121)\nplt.plot(x,y,'ro--')\nplt.xlabel('time')\nplt.ylabel('performance')\nplt.title(\"This is my title\")\nax = plt.subplot(122)\nplt.plot(x,y,'bo--')\nplt.xlabel('time')\nplt.ylabel('performance')\nplt.title(\"This is my title\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'This is my title')\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"tutorials/linux/01-linux-basics/","title":"Linux Basics","text":""},{"location":"tutorials/linux/01-linux-basics/#bash","title":"Bash","text":""},{"location":"tutorials/linux/01-linux-basics/#external-resources","title":"External Resources","text":"<ul> <li>https://cheatography.com/davechild/cheat-sheets/linux-command-line/</li> </ul>"},{"location":"tutorials/linux/01-linux-basics/#arguments","title":"Arguments","text":"<p>short arguments usually have a single dash followed by a single character.  An example is <code>ls -a</code>,  where <code>-a</code> is an additional flag/attribute that indicates to the <code>ls</code> command that you should list all files, even if hidden by a <code>.</code></p> <p>long arguments usually have two dashes followed by a word or phrase (connected by dashes). An example is <code>ls --all</code>,  where <code>--all</code> has the same meaning as <code>-a</code> above.</p>"},{"location":"tutorials/linux/01-linux-basics/#most-common-commands","title":"Most Common Commands","text":"command meaning example <code>&lt;command&gt; -h</code> display help <code>&lt;command&gt; --help</code> display help (long form of argumentz) <code>cd &lt;absolute or relative path&gt;</code> change directory to the path indicated <code>ls</code> list files in current directory <code>ls -la</code> list all files(<code>a</code>) in list format(<code>-l</code>) <code>chown &lt;username&gt;:&lt;groupname&gt; &lt;file or folder&gt;</code> change the owner of the file or folder <code>chown -R &lt;username&gt;:&lt;groupname&gt; &lt;file or folder&gt;</code> change the owner of the folder recursively <code>chmod &lt;0-7&gt;&lt;0-7&gt;&lt;0-7&gt; &lt;file or folder&gt;</code> change the owner:group:anyone permissions for a file <code>chmod -R &lt;0-7&gt;&lt;0-7&gt;&lt;0-7&gt; &lt;file or folder&gt;</code> change the owner:group:anyone permissions for a folder recursively <code>chmod +x &lt;file or folder&gt;</code> add execution permission to a file/folder <code>sudo+&lt;command&gt;</code> <code>mkdir &lt;path&gt;</code> make a directory at path indicated <code>mv &lt;path1&gt; &lt;path2&gt;</code> move a file from first path indicated to second path indicated <code>cp &lt;path1&gt; &lt;path2&gt;</code> copy files from first path indicated to second path indicated <code>cp -r &lt;path1&gt; &lt;path2&gt;</code> copy files recursively from first path indicated to second path indicated <code>rmdir &lt;path&gt;</code> delete a folder at a given path (folder must be empty) <code>rm &lt;path1&gt; ...</code> delete a file at a given path(s) <code>rm -rf &lt;path&gt; ...</code> delete a file/folder at a given path(s) using the <code>force</code> and <code>recursive</code> option (removes folders too, be careful!) ``````"},{"location":"tutorials/linux/01-linux-basics/#keystrokes","title":"Keystrokes","text":"key combineation meaning <code>ctrl+c</code> stops execution of whatever is running in bash <code>ctrl+alt+t</code> opens up a new <code>bash</code> terminal window"},{"location":"tutorials/opencv/","title":"Anaconda Python","text":"<ul> <li>as far as I know, doesn't play nice with ROS2</li> </ul>"},{"location":"tutorials/opencv/#installing","title":"Installing","text":"<ul> <li>Anaconda</li> <li>Packages<ul> <li>64-bit Linux</li> </ul> </li> </ul>"},{"location":"tutorials/opencv/#simple-conda-environment-environment","title":"Simple Conda Environment Environment","text":"<p>make sure you have already installed</p> <p>ffmpeg gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly</p> <pre><code>conda create -n cv\nconda activate cv\nconda install -y jupyter matplotlib numpy scipy\nconda install -y -c conda-forge opencv\npip install yt-dlp\n</code></pre> <pre><code>jupyter lab\n</code></pre> <p>Open and run your code</p> <p>Delete your environment</p> <pre><code>conda remove --name cv --all\n</code></pre>"},{"location":"tutorials/opencv/#yt-dlp","title":"yt-dlp","text":"<pre><code>pip install yt-dlp #apt is out of date\nyt-dlp -F https://www.youtube.com/watch?v=43r0hha2320\nyt-dlp -f &lt;num here&gt; https://www.youtube.com/watch?v=43r0hha2320\n</code></pre>"},{"location":"tutorials/opencv/#computer-vision","title":"Computer Vision","text":""},{"location":"tutorials/opencv/#reading-displaying-and-writing","title":"Reading, Displaying, and Writing","text":"<pre><code># import the cv2 library\nimport cv2\n\n# The function cv2.imread() is used to read an image.\nimg_grayscale = cv2.imread('test.jpg',0)\n# The function cv2.imshow() is used to display an image in a window.\ncv2.imshow('graycsale image',img_grayscale)\n# waitKey() waits for a key press to close the window and 0 specifies indefinite loop\ncv2.waitKey(0)\n# cv2.destroyAllWindows() simply destroys all the windows we created.\ncv2.destroyAllWindows()\n# The function cv2.imwrite() is used to write an image.\ncv2.imwrite('grayscale.jpg',img_grayscale)\n</code></pre> <p>:::{style=\"font-size:12pt\"} https://learnopencv.com/read-display-and-write-an-image-using-opencv/ :::</p>"},{"location":"tutorials/opencv/#imread","title":"Imread","text":"<pre><code>imread(filename, flags)\n</code></pre> <ol> <li>The first argument is the fully qualified image pathname</li> <li>The second argument lets you specify how the image should be represented. </li> </ol> <pre><code>cv2.IMREAD_UNCHANGED  or -1\ncv2.IMREAD_GRAYSCALE  or 0\ncv2.IMREAD_COLOR  or 1\n</code></pre> <p>:::{style=\"font-size:12pt\"} https://learnopencv.com/read-display-and-write-an-image-using-opencv/ :::</p>"},{"location":"tutorials/opencv/#imshow","title":"Imshow","text":"<pre><code>imshow(window_name, image)\n</code></pre> <ol> <li>The first argument is the window name that will be displayed on the window.</li> <li>The second argument is the image that you want to display. </li> </ol> <p>:::{style=\"font-size:18pt\"}</p> <p>Note: <code>waitKey(var)</code> is a keyboard-binding function where <code>var</code> is the time (in milliseconds) the window will be displayed. :::</p> <p>:::{style=\"font-size:12pt\"} https://learnopencv.com/read-display-and-write-an-image-using-opencv/ :::</p>"},{"location":"tutorials/opencv/#imwrite","title":"Imwrite","text":"<pre><code>imwrite(filename, image).\n</code></pre> <ol> <li>The first argument is the filename.  The conversion is performed automatically based on the extension</li> <li>The second argument is the image you want to save</li> </ol> <p>:::{style=\"font-size:12pt\"} https://learnopencv.com/read-display-and-write-an-image-using-opencv/ :::</p>"},{"location":"tutorials/opencv/#videocapture","title":"VideoCapture","text":"<p><code>cv2.VideoCapture(var)</code></p> <ul> <li>var can represent a camera index or a file path to a video</li> </ul>"},{"location":"tutorials/opencv/#cropping","title":"Cropping","text":"<ul> <li>Image is stored as a numpy array</li> <li>cropping is done just as you access subarrays</li> </ul> <pre><code># Cropping an image\ncropped_image = img[80:280, 150:330]\n</code></pre> <p>:::{style=\"font-size:12pt\"} - https://learnopencv.com/cropping-an-image-using-opencv/ - https://numpy.org/doc/stable/user/numpy-for-matlab-users.html :::</p>"},{"location":"tutorials/opencv/#rotation","title":"Rotation","text":"\\[M = \\begin{bmatrix} cos\\theta &amp; -sin\\theta \\\\ sin\\theta &amp; cos\\theta \\end{bmatrix} \\] <pre><code># dividing height and width by 2 to get the center of the image\nheight, width = image.shape[:2]\n# get the center coordinates of the image to create the 2D rotation matrix\ncenter = (width/2, height/2)\n# using cv2.getRotationMatrix2D() to get the rotation matrix\nrotate_matrix = cv2.getRotationMatrix2D(center=center, angle=45, scale=1)\n# rotate the image using cv2.warpAffine\nrotated_image = cv2.warpAffine(src=image, M=rotate_matrix, dsize=(width, height))\n</code></pre> <p>:::{style=\"font-size:12pt\"} - https://learnopencv.com/image-rotation-and-translation-using-opencv/ :::</p>"},{"location":"tutorials/opencv/#translation","title":"Translation","text":"\\[ M = \\begin{bmatrix} 1 &amp; 0 &amp; t_x \\\\ 0 &amp; 1 &amp; t_y \\end{bmatrix} \\] <pre><code>image = cv2.imread('image.jpg')\nheight, width = image.shape[:2]\nM = np.array([[1, 0, tx],[0, 1, ty]], dtype=np.float32)\nimage2 = cv2.warpAffine(src=image, M=M, dsize=(width, height))\n</code></pre> <p>:::{style=\"font-size:12pt\"} - https://learnopencv.com/image-rotation-and-translation-using-opencv/ :::</p>"},{"location":"tutorials/opencv/#color-spaces-segmentation","title":"Color Spaces &amp; Segmentation","text":"<ul> <li>Possible to switch between color spaces</li> <li>RGB, HSV, YCrCb, LAB</li> </ul> <pre><code>bright = cv2.imread('cube1.jpg')\nbrightHSV = cv2.cvtColor(bright, cv2.COLOR_BGR2HSV)\nbgr = [40, 158, 16]\nthresh = 40\nhsv = cv2.cvtColor( np.uint8([[bgr]] ), cv2.COLOR_BGR2HSV)[0][0]\nminHSV = np.array([hsv[0] - thresh, hsv[1] - thresh, hsv[2] - thresh])\nmaxHSV = np.array([hsv[0] + thresh, hsv[1] + thresh, hsv[2] + thresh])\nmaskHSV = cv2.inRange(brightHSV, minHSV, maxHSV)\nresultHSV = cv2.bitwise_and(brightHSV, brightHSV, mask = maskHSV)\n</code></pre>"},{"location":"tutorials/opencv/#thresholding","title":"Thresholding","text":"<ul> <li>Can use native numpy operatiors <code>&gt;</code> or <code>&lt;</code></li> <li>Can use more advanced thresholding function</li> </ul> <pre><code># import opencv\nimport cv2\n# Read image\nsrc = cv2.imread(\"threshold.png\", cv2.IMREAD_GRAYSCALE)\n# Set threshold and maxValue\nthresh = 0\nmaxValue = 255\n# Basic threshold example\nth, dst = cv2.threshold(src, thresh, maxValue, cv2.THRESH_BINARY);\n</code></pre>"},{"location":"tutorials/opencv/#filtering","title":"Filtering","text":"<ul> <li>discussed a bit last time</li> <li>concept of a kernel</li> </ul> <p>:::{style=\"font-size:12pt\"} https://learnopencv.com/image-filtering-using-convolution-in-opencv/ :::</p>"},{"location":"tutorials/opencv/#blob-detection","title":"Blob Detection","text":"<p>:::{style=\"font-size:12pt\"} https://www.delftstack.com/howto/python/opencv-blob-detection/</p> <p>This one didn't work for me... https://learnopencv.com/blob-detection-using-opencv-python-c/ :::</p>"},{"location":"tutorials/opencv/#edge-detection","title":"Edge Detection","text":"<p>:::{style=\"font-size:12pt\"} https://learnopencv.com/edge-detection-using-opencv/ :::</p>"},{"location":"tutorials/opencv/#contour-detection","title":"Contour Detection","text":"<p>:::{style=\"font-size:12pt\"} https://learnopencv.com/contour-detection-using-opencv-python-c/ :::</p>"},{"location":"tutorials/opencv/#background-estimation","title":"Background Estimation","text":"<p>:::{style=\"font-size:12pt\"} https://learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/ :::</p>"},{"location":"tutorials/ros/01-colcon-and-ws/","title":"01 colcon and ws","text":"<p>derived from here</p> <pre><code>mkdir -p ~/ros2_ws/src\ncd ~/ros2_ws\ngit clone https://github.com/ros2/examples src/examples -b galactic\ncolcon build --symlink-install\ncolcon test\n. install/setup.bash\n</code></pre>"},{"location":"tutorials/ros/01-colcon-and-ws/#colcon-cd-and-tab-complete","title":"Colcon cd and tab complete","text":"<ul> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-Client-Libraries/Colcon-Tutorial.html#setup-colcon-cd</li> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-Client-Libraries/Colcon-Tutorial.html#setup-colcon-tab-completion</li> </ul> <pre><code>echo \"source /usr/share/colcon_cd/function/colcon_cd.sh\" &gt;&gt; ~/.bashrc\necho \"export _colcon_cd_root=/opt/ros/galactic/\" &gt;&gt; ~/.bashrc\necho \"source /usr/share/colcon_argcomplete/hook/colcon-argcomplete.bash\" &gt;&gt; ~/.bashrc\n</code></pre> <pre><code>. ros2_ws/install/setup.bash\nros2 run examples_rclcpp_minimal_subscriber subscriber_member_function\n</code></pre> <pre><code>. ros2_ws/install/setup.bash\nros2 run examples_rclcpp_minimal_publisher publisher_member_function\n</code></pre>"},{"location":"tutorials/ros/01-colcon-and-ws/#workspace-setup","title":"Workspace setup","text":"<pre><code>cd ~/ros2_ws/src\ngit clone https://github.com/ros/ros_tutorials.git -b galactic-devel\ncd ..\nrosdep install -i --from-path src --rosdistro galactic -y\nsed -i \"s/setWindowTitle(\\\"TurtleSim\\\");/setWindowTitle(\\\"MyTurtleSim\\\");/\" ~/ros2_ws/src/ros_tutorials/turtlesim/src/turtle_frame.cpp\ncolcon build\n. ~/ros2_ws/install/setup.bash\nros2 run turtlesim turtlesim_node\n</code></pre> <p>compare with </p> <pre><code>ros2 run turtlesim turtlesim_node\n</code></pre>"},{"location":"tutorials/ros/01-colcon-and-ws/#creating-a-package","title":"Creating a Package","text":"<p>https://docs.ros.org/en/galactic/Tutorials/Beginner-Client-Libraries/Creating-Your-First-ROS2-Package.html https://docs.ros.org/en/galactic/Tutorials/Beginner-Client-Libraries/Colcon-Tutorial.html#create-your-own-package</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> <pre><code>ros2 pkg create --build-type ament_cmake &lt;package_name&gt;\nros2 pkg create --build-type ament_python &lt;package_name&gt;\n</code></pre> <pre><code>ros2 pkg create --build-type ament_cmake --node-name my_node my_package\nros2 pkg create --build-type ament_python --node-name my_node my_package\n</code></pre> <p>cd ~/ros2_ws colcon build # build everything colcon build --packages-select my_package # build just my package</p> <p>tree ~/ros2_ws/src/my_package/</p> <pre><code>/home/danaukes/ros2_ws/src/my_package/\n\u251c\u2500\u2500 my_package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my_node.py\n\u251c\u2500\u2500 package.xml\n\u251c\u2500\u2500 resource\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my_package\n\u251c\u2500\u2500 setup.cfg\n\u251c\u2500\u2500 setup.py\n\u2514\u2500\u2500 test\n    \u251c\u2500\u2500 test_copyright.py\n    \u251c\u2500\u2500 test_flake8.py\n    \u2514\u2500\u2500 test_pep257.py\n\n3 directories, 9 files\n</code></pre> <pre><code>nano ~/ros2_ws/src/my_package/package.xml\nnano ~/ros2_ws/src/my_package/setup.py\n</code></pre>"},{"location":"tutorials/ros/02-open-cv-subscriber/","title":"Creating a simple OpenCV Subscriber","text":"<p>derived from here with thanks!</p> <pre><code>python3 -m pip install --upgrade pip\npip install opencv-python\nsudo apt install ros-galactic-vision-opencv\n</code></pre> <pre><code># make a brand new workspace\nmkdir -p dan_ws/src\ncd dan_ws/src/\ncd src\nros2 pkg create --build-type ament_python opencv_testing --dependencies rclpy image_transport cv_bridge sensor_msgs std_msgs python3-opencv\ncd ..\nrosdep install -i --from-path src --rosdistro galactic -y\ncolcon build\nls\n. install/local_setup.bash\n</code></pre>"},{"location":"tutorials/ros/02-open-cv-subscriber/#create-subscriber","title":"create subscriber","text":"<p>paste in the following, all at once.</p> <pre><code>cat &lt;&lt; EOT | tee ~/dan_ws/src/opencv_testing/opencv_testing/image_subscriber.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image \nfrom cv_bridge import CvBridge\nimport cv2\n\nclass ImageSubscriber(Node):\n  def __init__(self):\n    super().__init__('subscriber')\n\n    self.subscription = self.create_subscription(Image,'/color/preview/image', self.listener_callback, 10)\n    self.br = CvBridge()\n\n  def listener_callback(self, data):\n\n    self.get_logger().info('Receiving video frame')\n    current_frame = self.br.imgmsg_to_cv2(data)\n    cv2.imshow(\"camera\", current_frame)\n    cv2.waitKey(1)\n\ndef main(args=None):\n\n  rclpy.init(args=args)\n  image_subscriber = ImageSubscriber()\n  rclpy.spin(image_subscriber)\n  image_subscriber.destroy_node()\n  rclpy.shutdown()\n\nif __name__ == '__main__':\n  main()\nEOT\n</code></pre> <pre><code>nano ~/dan_ws/src/opencv_testing/setup.py\n</code></pre> <p>edit the lines:</p> <pre><code>        'console_scripts': [\n        ],\n</code></pre> <p>to look like this:</p> <pre><code>        'console_scripts': [\n        'image_subscriber = opencv_testing.image_subscriber:main',\n        ],\n</code></pre>"},{"location":"tutorials/ros/02-open-cv-subscriber/#edit-packagexml","title":"Edit package.xml","text":"<pre><code>cd src/opencv_testing/\nnano package.xml \n</code></pre> <p>Replace the following lines in package.xml with your own description:</p> <pre><code>  &lt;description&gt;OpenCV image handling&lt;/description&gt;\n  &lt;maintainer email=\"danaukes@danaukes.com\"&gt;danaukes&lt;/maintainer&gt;\n  &lt;license&gt;MIT License&lt;/license&gt;\n</code></pre> <pre><code>cd ~/dan_ws\nrosdep install -i --from-path src --rosdistro galactic -y\ncolcon build\nros2 run opencv_testing image_subscriber\n</code></pre>"},{"location":"tutorials/ros/03-plotting/","title":"Plotting in Ros","text":"<p>https://docs.ros.org/en/galactic/Concepts/About-RQt.html</p> <pre><code>sudo apt install ros-galactic-rqt-plot\n</code></pre>"},{"location":"tutorials/ssh-and-networking/01-using-ssh-keys/","title":"Using SSH Keys","text":""},{"location":"tutorials/ssh-and-networking/01-using-ssh-keys/#create-the-keys","title":"Create the Keys","text":"<ol> <li>create a pubplic / private keypair named turtlebot and turtlebot.pub</li> </ol> <pre><code>ssh-keygen -t ed25519 -f .ssh/turtlebot\n</code></pre>"},{"location":"tutorials/ssh-and-networking/01-using-ssh-keys/#transferring-the-keys","title":"Transferring the Keys","text":"<ol> <li>copy the key to another computer.  It will prompt you for your password</li> </ol> <pre><code>ssh-copy-id -i ~/.ssh/turtlebot.pub &lt;user_name&gt;@&lt;ip-address-of-the-server&gt;\n</code></pre> <p>or </p> <pre><code>cat ~/.ssh/turtlebot.pub | ssh &lt;user_name&gt;@&lt;ip-address-of-the-server&gt; 'cat &gt;&gt; .ssh/authorized_keys'\n</code></pre> <p>remember that the default username for the turtlebot is <code>ubuntu</code></p> <p>Example:</p> <pre><code>ssh-keygen -t ed25519 -f ~/.ssh/turtlebot\nssh-copy-id -i ~/.ssh/turtlebot.pub ubuntu@turtlebot-lite-00.ts \nssh ubuntu@turtlebot-lite-00.ts -i .ssh/turtlebot\n</code></pre>"},{"location":"tutorials/ssh-and-networking/01-using-ssh-keys/#configuring-ssh-for-the-turtlebot","title":"Configuring SSH for the Turtlebot","text":"<ol> <li>define your turtlebot</li> </ol> <pre><code>$my_turtlebot_num=\"&lt;put in the number of your turtlebot here&gt;\"\n</code></pre> <ol> <li>add a configuration for using the key by default</li> </ol> <pre><code>cat &lt;&lt;EOT | tee -a ~/.ssh/config\n\nHost tbot$my_turtlebot_num\n   User ubuntu\n   IdentitiesOnly yes\n   Hostname turtlebot-lite-$my_turtlebot_num.ts\n   PreferredAuthentications publickey \n   IdentityFile ~/.ssh/turtlebot\n\nEOT\n</code></pre>"},{"location":"tutorials/ssh-and-networking/01-using-ssh-keys/#configuring-ssh-for-the-virtual-machine","title":"Configuring SSH for the Virtual Machine","text":"<ol> <li>Copy the public key to your virtual machine</li> </ol> <pre><code>ssh-copy-id -i ~/.ssh/turtlebot.pub danaukes-ros-virtual-1.ts \n</code></pre> <ol> <li>create a ssh configuration to use the key by default</li> </ol> <pre><code>cat &lt;&lt;EOT | tee -a ~/.ssh/config\n\nHost virt0\n   User &lt;put the user name you use to connect to your virtual machine here&gt;\n   IdentitiesOnly yes\n   Hostname &lt;put the host name of your virtual machine here&gt;\n   PreferredAuthentications publickey \n   IdentityFile ~/.ssh/turtlebot\n\nEOT\n</code></pre>"},{"location":"tutorials/ssh-and-networking/01-using-ssh-keys/#external-resources","title":"External Resources","text":"<ul> <li>https://www.howtogeek.com/168147/add-public-ssh-key-to-remote-server-in-a-single-command/</li> <li>https://linuxhandbook.com/add-ssh-public-key-to-server/</li> </ul>"},{"location":"tutorials/ssh-and-networking/02-using-hosts-file/","title":"Using Ubuntu's Hosts File","text":""},{"location":"tutorials/ssh-and-networking/02-using-hosts-file/#general-steps","title":"General Steps","text":"<p>Ubuntu's <code>/etc/hosts</code> file is used to locally define mappings between computer names and ip addresses on your computer.  By adding entries to the hosts file, you can add your own computers to the list of computers with names pre-defined</p> <ol> <li>backup a copy of the hosts file</li> </ol> <pre><code>sudo cp /etc/hosts /etc/hosts.bak\n</code></pre> <ol> <li>Obtain the ip addresses of computers on your tailscale network</li> </ol> <pre><code>tailscale status\n</code></pre> <ol> <li>edit your hosts file</li> </ol> <pre><code>sudo nano /etc/hosts\n</code></pre> <ol> <li> <p>Add entries.  It is probably a good idea to add your custom entries to the end of the file and to provide a spacer of comment characters (<code>#</code>) to indicate your edits.</p> <p>For example:</p> </li> </ol> <pre><code>####################\n\n100.84.161.123 turtlebot-lite-00.ts\n100.85.231.321 danaukes-ros-virtual-1.ts \n...\n\n</code></pre>"},{"location":"tutorials/ssh-and-networking/02-using-hosts-file/#updating-hosts-from-tailscale","title":"Updating hosts from tailscale","text":"<p>after installing tailscale, </p> <pre><code>my_turtlebot_num=\"&lt;put in the number of your turtlebot here&gt;\"\n</code></pre> <pre><code>cp /etc/hosts ~/hosts.bak\necho \"$(tailscale ip --4 turtlebot-lite-$my_turtlebot_num) turtlebot-lite-$my_turtlebot_num.ts\" | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"tutorials/ssh-and-networking/03-dds-options/","title":"03 dds options","text":""},{"location":"tutorials/ssh-and-networking/03-dds-options/#introduction","title":"Introduction","text":"<p>Here is a summary of hardware information and the challenges of implementing ROS2 and DDS on a school network</p>"},{"location":"tutorials/ssh-and-networking/03-dds-options/#student-virtual-machines-on-ros2","title":"Student virtual machines on ROS2","text":"<ul> <li>uses DDS-RTPS as the communication layer</li> <li>different vendors of DDS can be used in theory</li> <li>depending on the hardware below, only a couple options are available in reality</li> <li>require bridged adapter mode with unique ip address...how to do this on a virtual machine on WiFi?</li> <li>need wifi connectivity</li> <li>need to anticipate up to 40-50 students max</li> </ul>"},{"location":"tutorials/ssh-and-networking/03-dds-options/#turtlebot-4","title":"Turtlebot 4","text":"<p>Consists of the devices below I anticipate 15 turtlebots max</p>"},{"location":"tutorials/ssh-and-networking/03-dds-options/#irobot-create3","title":"Irobot Create3","text":"<ul> <li>this is the platform for the mobile robots we have</li> <li>only has a 2.4 Ghz radio: https://iroboteducation.github.io/create3_docs/setup/network-config/</li> <li>time synchronization requires the internet or manual set every power cycle, which is not feasible.</li> <li>only supports a subset of DDS vendors, fast-dds and cyclone-dds: https://iroboteducation.github.io/create3_docs/setup/xml-config/</li> <li>finding MAC addresses of these devices in advance is time-consuming but they should not change</li> </ul>"},{"location":"tutorials/ssh-and-networking/03-dds-options/#raspberry-pi-on-the-turtlebot","title":"Raspberry pi on the Turtlebot","text":"<ul> <li>connects to other sensors: depth camera, lidar, others</li> <li>runs ubuntu 20 (this year), 22/24 next time we run the class probably</li> <li>requires 5GHz in practice for high throughput data streams</li> <li>in theory supports a broader range of DDS/RTPS clients.  The client type is hard-coded in a couple different configs and in practice it's hard to config and test</li> <li>needs internet access for package updates (images shipped on the turtlebot were DOA this semester)</li> </ul>"},{"location":"tutorials/ssh-and-networking/03-dds-options/#dds-rtps","title":"DDS / RTPS","text":"<p>There is some information on setting up static dds so that discovery which uses udp multicast is not needed:</p> <ul> <li>https://fast-dds.docs.eprosima.com/en/latest/fastdds/use_cases/well_known_deployments/well_known_deployments.html</li> <li>https://stackoverflow.com/questions/52092597/is-multicasting-necessary-for-dds-based-communication</li> <li>https://cyclonedds.io/docs/cyclonedds/latest/config/multicasting.html</li> <li>https://cyclonedds.io/docs/cyclonedds/latest/config/network_interfaces.html</li> <li>https://answers.ros.org/question/412578/disable-multicast-and-enable-unicast-for-cyclone-dds/</li> <li>https://answers.ros.org/question/342191/how-to-disable-multicast-in-cyclone-dds/</li> </ul> <p>unfortunately, information about actually implementing custom dds configs on the Create3, virtual machine, and raspberry pi is scarce</p> <p>We need a solution where</p> <ul> <li>all students can see each other's pcs on ROS2</li> <li>students who may come and go with their devices can be visible on the network</li> <li>high data throughput for camera data and that many computers talking to each other.</li> </ul> <p>Solutions that require hard-coded configurations will be difficult to implement in a classroom scenario, because people come and go all the time, and any at-scale, real-world testing can only really be done when all the students and their computers are there, and then it's too late.</p>"},{"location":"tutorials/ssh-and-networking/03-dds-options/#turtlebot4","title":"Turtlebot4","text":"<ul> <li>https://github.com/turtlebot/turtlebot4/issues/17</li> <li>https://github.com/turtlebot/turtlebot4/issues/26</li> <li>https://gist.github.com/roni-kreinin/8fbb20cb4603b8eb5e3961167fb22cd4</li> </ul>"},{"location":"tutorials/ssh-and-networking/03-dds-options/#ros","title":"ROS","text":"<ul> <li>Concepts</li> <li>DDS implementations</li> <li>About different ROS 2 DDS/RTPS vendors</li> <li>About ROS 2 middleware implementations</li> <li>Working with multiple ROS 2 middleware implementations</li> <li>ROS on DDS (old article with motivation)</li> </ul>"},{"location":"tutorials/ssh-and-networking/03-dds-options/#create3","title":"Create3","text":"<ul> <li>Using multiple Create\u00ae 3 robots</li> <li>Network Recommendations</li> <li>ROS 2 Middleware (RMW) Configuration</li> <li>Install ROS 2 Galactic with Create 3 Messages on an Ubuntu 20.04 Machine</li> <li>Connect Create\u00ae 3 to Wi-Fi</li> </ul>"},{"location":"tutorials/ssh-and-networking/03-dds-options/#fast-dds","title":"Fast DDS","text":"<ul> <li>FastDDS Network Deployments</li> </ul>"},{"location":"tutorials/ssh-and-networking/03-dds-options/#misc","title":"Misc","text":""},{"location":"tutorials/ssh-and-networking/04-unicast-with-cyclone/","title":"Unicast with CycloneDDS","text":""},{"location":"tutorials/ssh-and-networking/04-unicast-with-cyclone/#raspberry-pi","title":"Raspberry pi","text":"<pre><code>&lt;CycloneDDS&gt;\n    &lt;Domain&gt;\n        &lt;General&gt;\n&lt;!--            &lt;NetworkInterfaceAddress&gt;wlan0&lt;/NetworkInterfaceAddress&gt; DEPRECATED--&gt;\n            &lt;AllowMulticast&gt;false&lt;/AllowMulticast&gt;\n            &lt;EnableMulticastLoopback&gt;true&lt;/EnableMulticastLoopback&gt;\n        &lt;/General&gt;\n        &lt;Discovery&gt;\n          &lt;ParticipantIndex&gt;auto&lt;/ParticipantIndex&gt;\n          &lt;Peers&gt;\n            &lt;Peer Address=\"10.0.10.10\"/&gt;\n            &lt;Peer Address=\"10.0.10.11\"/&gt;\n          &lt;/Peers&gt;\n    &lt;/Discovery&gt;\n    &lt;/Domain&gt;\n&lt;/CycloneDDS&gt;\n</code></pre>"},{"location":"tutorials/ssh-and-networking/04-unicast-with-cyclone/#virtualbox","title":"Virtualbox","text":"<pre><code>&lt;CycloneDDS&gt;\n    &lt;Domain&gt;\n        &lt;General&gt;\n            &lt;DontRoute&gt;true&lt;/DontRoute&gt;        \n&lt;!--            &lt;NetworkInterfaceAddress&gt;enp0s3&lt;/NetworkInterfaceAddress&gt; DEPRECATED--&gt;\n            &lt;AllowMulticast&gt;false&lt;/AllowMulticast&gt;\n            &lt;EnableMulticastLoopback&gt;true&lt;/EnableMulticastLoopback&gt;\n        &lt;/General&gt;\n        &lt;Discovery&gt;\n          &lt;ParticipantIndex&gt;auto&lt;/ParticipantIndex&gt;\n          &lt;Peers&gt;\n            &lt;Peer Address=\"192.168.4.218\"/&gt;\n            &lt;Peer Address=\"192.168.4.198\"/&gt;\n          &lt;/Peers&gt;\n    &lt;/Discovery&gt;\n    &lt;/Domain&gt;\n&lt;/CycloneDDS&gt;\n</code></pre> <p>not sure if the <code>&lt;DontRoute&gt;true&lt;/DontRoute&gt;</code> is needed here.</p>"},{"location":"tutorials/ssh-and-networking/04-unicast-with-cyclone/#external-links","title":"External Links","text":"<ul> <li>https://iroboteducation.github.io/create3_docs/setup/xml-config/</li> <li>https://stackoverflow.com/questions/64352350/can-dds-protocol-be-used-to-communicate-between-devices-connected-to-different-n</li> </ul>"},{"location":"tutorials/turtlebot/01-turtlebot-unboxing/","title":"Unboxing the Turtlebot","text":""},{"location":"tutorials/turtlebot/01-turtlebot-unboxing/#initial-steps","title":"Initial Steps","text":"<p>This kit will be used many times, so please preserve all packaging materials, including the box and all cardboard!  Store in a clean, dry place.</p> <ol> <li> <p>Carefully open the turtlebot box.  Don't use a blade to cut open the box.</p> <p> </p> <p>Take care to save all documentation that comes with the box IN the box</p> <p> </p> </li> <li> <p>Set the turtlebot on a flat surface</p> <p> </p> <p>Liners, cushions, and dessicants can be disposed of</p> <p> </p> </li> <li> <p>Peel the protective stickers off the camera    </p> <p> </p> </li> <li> <p>Store the box in a clean, dry place for the duration of use.  We will ask for the turtlebot to be returned in its original box with all packaging materials.</p> </li> </ol>"},{"location":"tutorials/turtlebot/01-turtlebot-unboxing/#label","title":"Label","text":"<p>Add a label to your turtlebot to identify it (supplied in class)</p>"},{"location":"tutorials/turtlebot/01-turtlebot-unboxing/#optional-backing-up-the-sd-card","title":"(optional) Backing up the SD Card","text":"<ol> <li> <p>Open the tray part-way, being careful not to strain the USB cables</p> <p> </p> </li> <li> <p>Note which cable goes to which USB port.  (The high-speed ports should be reserved for high-speed data needs).  Unplug the cables, pull the tray out further, and unplug the power cable.</p> <p> </p> </li> <li> <p>Remove the included SD card and back it up using Balena Etcher (or similar).  This can be used to bring the turtlebot back to its initial state.  </p> <p> </p> <p>This is a 32Gb file, so please ensure you have room to store it first.</p> </li> <li> <p>Once the backup is complete, return the SD Card to its slot, plug the USB and power cables back in, and close the tray.</p> </li> </ol>"},{"location":"tutorials/turtlebot/01-turtlebot-unboxing/#charging","title":"Charging","text":"<ol> <li> <p>Plug the cord into the included power base and rest the create3 module in it for charging.</p> <p> </p> </li> </ol>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/","title":"Turtlebot Config","text":""},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#intro","title":"Intro","text":"<p>Note, this tutorial is considered a companion to the following tutorials.  It is highly suggested to read through the turtlebot 4 quickstart first:</p> <ul> <li>https://turtlebot.github.io/turtlebot4-user-manual/overview/quick_start.html</li> <li>https://iroboteducation.github.io/create3_docs/</li> </ul>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#before-connecting-to-the-internet","title":"Before connecting to the internet","text":"<p>It's important to disable automatic updates as you should initiate it according to your own availability.  Otherwise, it will start updating the  first time you get an internet connection.  To prevent that, uninstall the \"unattended-upgrades\" package.</p> <pre><code>sudo apt remove unattended-upgrades\n</code></pre> <p>if you get a message like:</p> <pre><code>Waiting for cache lock: Could not get lock /var/lib/dpkg/lock-frontend. It is held by process 2131 (uWaiting for cache lock: Could not get lock /var/lib/dpkg/lock-frontend. It is held by process 2131 (unattended-upgr)   \n</code></pre> <pre><code>sudo killall unattended-upgr\n</code></pre> <p>and try again</p>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#update-wifi-access-point-name","title":"Update wifi access point name","text":"<p>For the purposes of the class at this point, we don't have a classroom wifi solution that works across all machines yet.  For that reason, please don't connect the turtlebot's raspberry pi to a wifi access point in the classroom yet.  Instead, create a custom access point name.</p> <p>https://www.linux.com/topic/distributions/how-use-netplan-network-configuration-tool-linux/</p> <p>back up your original netplan</p> <pre><code>sudo cp /etc/netplan/50-cloud-init.yaml /etc/netplan/50-cloud-init.yaml.bak\n</code></pre> <p>then edit the file</p> <pre><code>sudo nano /etc/netplan/50-cloud-init.yaml\n</code></pre> <p>edit the access point information, changing the SSID from <code>Turtlebot4</code> to <code>Turtlebot4-&lt;XY&gt;</code>,  where <code>&lt;XY&gt;</code> is your turtlebot's number (e.g. 01, 02, 03).  </p> <p>See lecture 5 notes for setting band and channel information.  You should set it to the 2.4Ghz band so that the create 3 can connect, and a channel that is appropriate for the classroom.</p> <p>To test, </p> <pre><code>sudo netplan try\n</code></pre> <p>try looking for the new access point name in the next 120 seconds.</p> <p>Otherwise, if you are happy with the settings you can write</p> <pre><code>sudo netplan apply\n</code></pre> <p>or simply reboot the computer</p> <pre><code>sudo reboot now\n</code></pre>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#optional-wifi-script","title":"(optional) Wifi Script","text":"<p>The turtlebot also comes with a wifi setup script. To reset your wifi, you can use <code>. /usr/local/bin/wifi.sh --help</code> to either reset wifi as an access point or set up wifi to connect to your router.</p> <p>To reset your raspberry pi into access point mode:</p> <pre><code>sudo wifi.sh -a # to reset\n</code></pre> <p>or, to map it to an existing wifi access point,</p> <pre><code>#sudo bash /usr/local/bin/wifi.sh -s '&lt;WIFI_SSID&gt;' -p '&lt;WIFI_PASSWORD&gt;' -r &lt;REGULATORY_DOMAIN&gt; &amp;&amp; sudo reboot\nsudo bash /usr/local/bin/wifi.sh -s 'idealab' -p 'origamiorigami' -r US &amp;&amp; sudo reboot\n</code></pre> <p>source: turtlebot manual</p>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#update-hostname","title":"Update Hostname","text":"<p>check your current hostname</p> <pre><code>cat /etc/hostname\n</code></pre> <p>paste in the following, where <code>&lt;XY&gt;</code> is your turtlebot's number (e.g. 01, 02, 03)</p> <pre><code>#echo \"turtlebot-lite-&lt;XY&gt;\" | sudo tee /etc/hostname \n#for example,\necho \"turtlebot-lite-00\" | sudo tee /etc/hostname \n</code></pre>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#remove-host-keys-and-regenerate","title":"remove host keys and regenerate","text":"<p>All the turtlebots will have the same ssh host keys, which can get confusing.</p> <pre><code>sudo rm /etc/ssh/ssh_host*\nsudo ssh-keygen -A\n</code></pre> <p>exit ssh and log back in follow instructions in error message</p> <pre><code>ssh-keygen -f \"/home/danaukes/.ssh/known_hosts\" -R \"10.0.10.11\"\n</code></pre> <p>sign back in</p>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#create3-configuration","title":"Create3 Configuration","text":"<p>Connnect to the Create3 with these instructions</p>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#generic-raspberry-pi-step-only-do-this-step-if-you-are-installing-to-a-plain-raspberry-pi","title":"Generic Raspberry Pi Step (Only do this step if you are installing to a plain raspberry pi:)","text":"<ol> <li>Remove USB from the cyclone dds config</li> </ol> <p>sudo nano /etc/cyclonedds_rpi.xml</p> <p>original</p> <pre><code>&lt;CycloneDDS&gt;\n    &lt;Domain&gt;\n        &lt;General&gt;\n&lt;!--            &lt;NetworkInterfaceAddress&gt;wlan0,usb0&lt;/NetworkInterfaceAddress&gt; DEPRECATED--&gt;\n        &lt;/General&gt;\n    &lt;/Domain&gt;\n&lt;/CycloneDDS&gt;\n</code></pre> <p>change <code>&lt;NetworkInterfaceAddress&gt;wlan0,usb0&lt;/NetworkInterfaceAddress&gt;</code></p> <p>to <code>&lt;NetworkInterfaceAddress&gt;wlan0&lt;/NetworkInterfaceAddress&gt;</code></p> <p>Note: NetworkInterfaceAddress is deprecated in modern cyclonedds (Humble and later): https://github.com/eclipse-cyclonedds/cyclonedds/issues/1434, but this implementation may not yet have made it to the create3</p>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#update-create3","title":"Update Create3","text":"<p>https://iroboteducation.github.io/create3_docs/releases/</p> <p>make sure you select the latest galactic update</p>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#make-a-bin-folder-and-a-helpful-bash-script","title":"Make a bin folder and a helpful bash script","text":"<pre><code>mkdir ~/bin\ncd ~/bin\ncat &lt;&lt; EOT | tee lidar_off\n#!/usr/bin/bash\nros2 service call /stop_motor std_srvs/srv/Empty {}\nEOT\nchmod +x lidar_off\necho \"export PATH=\\$PATH:$PWD\" &gt;&gt; ~/.bashrc\ncd\nsource ~/.bashrc\n\ncd ~/bin\ncat &lt;&lt; EOT | tee lidar_on\n#!/usr/bin/bash\nros2 service call /start_motor std_srvs/srv/Empty {}\nEOT\nchmod +x lidar_on\ncd\n\ncd ~/bin\ncat &lt;&lt; EOT | tee turtlebot_off\n#!/usr/bin/bash\nros2 service call /robot_power irobot_create_msgs/srv/RobotPower {}\nEOT\nchmod +x turtlebot_off\ncd\n</code></pre> <p>The following instructions are only valid if you are connected to the internet</p>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#update-software","title":"Update software","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#install-tailscale","title":"Install Tailscale","text":"<ol> <li>install tailscale</li> </ol> <pre><code>curl -fsSL https://tailscale.com/install.sh | sh\nsudo tailscale up\n</code></pre>"},{"location":"tutorials/turtlebot/02-turtlebot-configuration/#install-other-prerequisites","title":"Install other prerequisites","text":"<ol> <li>Install tmux</li> </ol> <pre><code>sudo apt install tmux\n</code></pre> <pre><code>sudo rosdep init &amp;&amp; rosdep update\n</code></pre> <p>note: may need to force install of galactic</p>"},{"location":"tutorials/turtlebot/03-usb-control/","title":"Controlling Raspberry Pi's USB Hub Controller","text":"<p>make and install the project</p> <pre><code>sudo apt-get install libusb-1.0-0-dev\ngit clone https://github.com/mvp/uhubctl\ncd uhubctl\nmake\nsudo make install\n</code></pre> <p>To turn off USB</p> <pre><code>sudo uhubctl -l 2 -a 0\n</code></pre> <p>To turn on USB</p> <pre><code>sudo uhubctl -l 2 -a 1\n</code></pre>"},{"location":"tutorials/turtlebot/04-oak-d-camera/","title":"Oak-D Camera information","text":"<p>https://robofoundry.medium.com/oak-d-lite-camera-ros2-setup-1e74ed03350d</p>"},{"location":"tutorials/virtual%20machine/01-installing-ubuntu-20-on-virtualbox/","title":"01 - Installing Ubuntu 20 on a Virtualbox guest","text":"<ol> <li> <p>Open Virtualbox and create a new machine</p> <p> </p> </li> <li> <p>Give the machine a name like \"ubuntu20\"</p> <p> </p> </li> <li> <p>Assign somewhere betweeen 4096 and 8192 Mb of memory to the virtual machine     </p> <p>Stay out of the red region, as this indicates your host machine will not have enough memory to run basic tasks</p> </li> <li> <p>Create a hard disk. Settings:</p> <ul> <li>Use a .vdi</li> <li>select \"dynamically allocated</li> <li>select at least 50Gb (100Gb recommended if possible)</li> </ul> <p> </p> <p>Because you have selected a \"dynamically allocated\" hard disk, even if you select a large drive size, it will not consume the entire file unless/until you use all that space.</p> </li> <li> <p>The machine is created.  Open machine settings.</p> </li> <li> <p> </p> </li> <li> <p>In the system --&gt; processor tab, select 2-4 cpus if possible, keeping the slider out of the red region.</p> <p> </p> </li> <li> <p>In the Storage tab, select the Ubuntu .iso image corresponding to Ubutnu 20.04 (you must download this from ubuntu.com)</p> <p> </p> </li> <li> <p>Set up Networking as seen below</p> <p></p> <p>Suggestion: Select your host's Wifi Card so that you can connect your virtual machine to whichever wireless network your host has attached to.</p> </li> <li> <p>Set up USB as seen below:</p> <p></p> </li> <li> <p>Set up Sharing.  Go to \"Shared Folders\" and create a new \"machine folder\" share.  Proceed as seen below.</p> <p> </p> </li> <li> <p>Start the Virtual Machine</p> <p></p> </li> <li> <p>Follow the installation steps below:</p> <p> </p> </li> <li> <p>Put in a unique machine name (\"your server's name\") that includes your name or initials</p> <p></p> </li> <li> <p>Continue configuring your setup</p> <p> </p> </li> <li> <p>Deselect Docker</p> <p> </p> </li> <li> <p>Continue configuring your setup</p> <p></p> </li> <li> <p>Reboot and Log in using the credentials you just created.</p> <p> </p> </li> <li> <p>update and upgrade your system using <code>sudo apt update &amp;&amp; sudo apt upgrade -y</code> </p> <p>Let the process finish</p> <p> </p> </li> <li> <p>Take a snapshot of your virtual machine at this point</p> <p> </p> </li> </ol>"},{"location":"tutorials/virtual%20machine/01-installing-ubuntu-20-on-virtualbox/#install-ubuntu-desktop","title":"Install Ubuntu Desktop","text":"<p>This will be necessary to view any ROS windows or data from your virtual machine</p> <ol> <li>Restart your virtual machine, open a terminal and type</li> </ol> <pre><code>sudo apt install ubuntu-desktop-minimal\n</code></pre> <p>Note: It has been reported that <code>ubuntu-desktop</code> is required in order to get the terminal program installed.</p>"},{"location":"tutorials/virtual%20machine/01-installing-ubuntu-20-on-virtualbox/#install-virtual-machine-additions","title":"Install Virtual Machine Additions","text":"<pre><code>sudo apt install gcc make perl build-essential\n</code></pre> <p>with your desktop installed and your virtual machine running, </p> <ol> <li>go to \"devices\" --&gt; \"install Guest Additions CD image\"</li> <li>select yes to autorun</li> </ol>"},{"location":"tutorials/virtual%20machine/02-installing-ros-and-configuring-your-environment/01-installing-ros/","title":"Installing ROS2 (galactic) on Ubuntu 20.04","text":"<ol> <li> <p>If running in a virtual machine, save a snapshot prior to installation</p> </li> <li> <p>Install Ros using the tutorial</p> <ol> <li> <p>Use this tutorial  to install ROS</p> </li> <li> <p>make sure you install both ros-galactic-desktop and ros-dev-tools:</p> <pre><code>sudo apt install ros-galactic-desktop\nsudo apt install ros-dev-tools\n</code></pre> </li> <li> <p>add ros to your .bashrc file</p> <pre><code>echo \"source /opt/ros/galactic/setup.bash\" &gt;&gt; ~/.bashrc \nsource .bashrc\n</code></pre> </li> </ol> </li> <li> <p>Test the installation</p> </li> </ol> <pre><code>ros2 run demo_nodes_cpp talker\n</code></pre> <p>in another ssh shell:</p> <pre><code>ros2 run demo_nodes_py listener\n</code></pre> <ol> <li>Install <code>rosdep</code></li> </ol> <p>More info</p> <pre><code>sudo apt-get install python3-rosdep\nsudo rosdep init\nrosdep update\n</code></pre> <ol> <li>Install cyclone DDS</li> </ol> <p>More info here</p> <pre><code>sudo apt install ros-galactic-rmw-cyclonedds-cpp\n</code></pre> <ol> <li>Shut down the computer</li> </ol> <pre><code>sudo shutdown now\n</code></pre> <ol> <li>If running in a virtual machine, save a snapshot at this point.</li> </ol>"},{"location":"tutorials/virtual%20machine/02-installing-ros-and-configuring-your-environment/01-installing-ros/#external-resources","title":"External Resources","text":"<ul> <li>https://turtlebot.github.io/turtlebot4-user-manual/overview/quick_start.html#installing-ros2-galactic-on-your-pc</li> </ul>"},{"location":"tutorials/virtual%20machine/02-installing-ros-and-configuring-your-environment/02-configuring-your-environment/","title":"Installing and Configuring your ROS2 Environment in a VirtualBox Guest","text":"<p>If you haven't yet installed ROS, see the previous tutorial</p>"},{"location":"tutorials/virtual%20machine/02-installing-ros-and-configuring-your-environment/02-configuring-your-environment/#external-resources","title":"External Resources","text":"<p>This information is derived from</p> <ul> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-CLI-Tools/Configuring-ROS2-Environment.html</li> <li>https://turtlebot.github.io/turtlebot4-user-manual/overview/quick_start.html#installing-ros2-galactic-on-your-pc</li> </ul>"},{"location":"tutorials/virtual%20machine/02-installing-ros-and-configuring-your-environment/02-configuring-your-environment/#essential-tools","title":"Essential Tools","text":"<pre><code>sudo apt install net-tools -y\n</code></pre> <pre><code>ifconfig\n</code></pre> <pre><code>curl -fsSL https://tailscale.com/install.sh | sh\n</code></pre> <pre><code>sudo apt install -y tmux\n</code></pre>"},{"location":"tutorials/virtual%20machine/02-installing-ros-and-configuring-your-environment/02-configuring-your-environment/#set-additional-environment-variables","title":"Set Additional Environment Variables","text":"<ul> <li>https://docs.ros.org/en/galactic/Tutorials/Beginner-CLI-Tools/Configuring-ROS2-Environment.html#the-ros-domain-id-variable</li> </ul> <pre><code>echo \"export ROS_DOMAIN_ID=0\" &gt;&gt; ~/.bashrc\necho \"export ROS_LOCALHOST_ONLY=0\" &gt;&gt; ~/.bashrc\n</code></pre> <p>more detailed information on domain id's can be found here</p> <p>working with multiple robots:</p> <ul> <li>https://turtlebot.github.io/turtlebot4-user-manual/tutorials/multiple_robots.html</li> </ul> <p>On other devices:</p> <ul> <li>https://github.com/micro-ROS/micro_ros_stm32cubemx_utils/issues/23</li> <li>https://integration-service.docs.eprosima.com/en/latest/examples/same_protocol/ros2_change_domain.html</li> <li>https://foxglove.dev/docs/studio/connection/ros2</li> <li>https://docs.ros.org/en/galactic/Concepts/About-Domain-ID.html</li> </ul>"},{"location":"tutorials/virtual%20machine/02-installing-ros-and-configuring-your-environment/02-configuring-your-environment/#install-other-packages","title":"Install Other Packages","text":"<pre><code>sudo apt update &amp;&amp; sudo apt install -y \\\n  build-essential \\\n  cmake \\\n  git \\\n  python3-colcon-common-extensions \\\n  python3-flake8 \\\n  python3-pip \\\n  python3-pytest-cov \\\n  python3-rosdep \\\n  python3-setuptools \\\n  python3-vcstool \\\n  wget\n</code></pre>"},{"location":"tutorials/virtual%20machine/02-installing-ros-and-configuring-your-environment/02-configuring-your-environment/#configure-cyclonedds","title":"Configure CycloneDDS","text":"<ol> <li>Run  <code>ip link</code> to get a list of virtualbox network interfaces.  In a virtualbox guest you will probably will end up with  <code>enp0s3</code> as your default network interface.  If that is the case, copy this code in to a terminal:</li> </ol> <pre><code>cat &lt;&lt;EOT &gt;&gt; ~/cyclonedds_pc.xml\n&lt;CycloneDDS&gt;\n    &lt;Domain&gt;\n        &lt;General&gt;\n            &lt;DontRoute&gt;true&lt;/DontRoute&gt;\n&lt;!--            &lt;NetworkInterfaceAddress&gt;enp0s3&lt;/NetworkInterfaceAddress&gt; DEPRECATED--&gt;\n        &lt;/General&gt;\n    &lt;/Domain&gt;\n&lt;/CycloneDDS&gt;\nEOT\nsudo mv ~/cyclonedds_pc.xml /etc/\necho \"export CYCLONEDDS_URI=/etc/cyclonedds_pc.xml\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>Note: NetworkInterfaceAddress is deprecated in modern cyclonedds (Humble and later): https://github.com/eclipse-cyclonedds/cyclonedds/issues/1434, but this implementation may not yet have made it to the create3</p> <ol> <li> <p>Declare your DDS implementation</p> <p>Derived from here: https://docs.ros.org/en/galactic/Installation/DDS-Implementations/Working-with-Eclipse-CycloneDDS.html#switch-to-rmw-cyclonedds</p> <pre><code>echo \"export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp\" &gt;&gt; ~/.bashrc\n</code></pre> <p>If multicast is not supported on your network, check out this link</p> </li> </ol>"},{"location":"tutorials/virtual%20machine/02-installing-ros-and-configuring-your-environment/02-configuring-your-environment/#set-up-git","title":"Set up Git","text":"<ol> <li>install and configure git with the following lines of code, substituting in your name and email address</li> </ol> <pre><code>sudo apt install -y git\ngit config --global user.email \"&lt;email@address.com&gt;\"\ngit config --global user.name \"&lt;First Last&gt;\"\ngit config --global pull.rebase false\ngit config --global advice.addIgnoredFile false\n</code></pre> <ol> <li>For example:    </li> </ol> <pre><code>sudo apt install -y git\ngit config --global user.email \"danaukes@danaukes.com\"\ngit config --global user.name \"Dan Aukes\"\ngit config --global pull.rebase false\ngit config --global advice.addIgnoredFile false\n</code></pre>"},{"location":"tutorials/virtual%20machine/02-installing-ros-and-configuring-your-environment/02-configuring-your-environment/#install-some-turtlebot4-specific-packages","title":"Install some Turtlebot4-specific packages","text":"<pre><code>sudo apt update\nsudo apt install -y ros-galactic-turtlebot4-desktop\nsudo apt install -y ros-galactic-turtlebot4-navigation\n</code></pre>"},{"location":"tutorials/virtual%20machine/misc/controlling-your-vm/","title":"Controlling your VM from the Terminal","text":"<p>You can command your virtual machine from the terminal.</p> <pre><code>vboxmanage startvm &lt;machinename&gt; --type headless\n</code></pre> <p>For example, if your virtual machine name is \"ubuntu20\", </p> <pre><code>vboxmanage startvm ubuntu20 --type headless\n</code></pre> <p>Other commands</p> <pre><code>VBoxManage controlvm \"ubuntu20\" poweroff\n</code></pre>"}]}